{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yurong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import javalang\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function：\n",
    "    Input the code token list, comment string，and return whether it's invalid\n",
    "The rule of valid method:\n",
    "    1. code token size <= 100\n",
    "    2. One-sentence-comment（I hope model generate only one sentence.\n",
    "       If training data consist multi-sentence comment, the effect will be bad and the first sentence only can not\n",
    "       properly describe the functionality of the method）\n",
    "\n",
    "PS: I regard \"tester\", \"setter\", \"getter\" and \"constructor\" as valid method\n",
    "'''\n",
    "def is_invalid_method(token_len, nl):   \n",
    "    if token_len > 100:\n",
    "        return True\n",
    "    if len(nl.split('.')) != 1 or len(nltk.word_tokenize(nl)) > 30:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "Function: \n",
    "    Input the root of AST and the deep of the tree, \n",
    "    it will filter the null value and return the list of SBT (structural-based travesal) and print the tree structure\n",
    "'''\n",
    "def parse_tree(root, deep):\n",
    "\n",
    "    seq = []\n",
    "    seq.extend(['(', str(root).split('(')[0]])\n",
    "    #print('\\t'*(deep)+str(root).split('(')[0])    # show node name\n",
    "    for attr in root.attrs:\n",
    "        if eval('root.%s' % attr) in [None, [], \"\", set(), False]:    # filter the null attr\n",
    "            continue\n",
    "        elif isinstance(eval('root.%s' % attr), list):\n",
    "            x = eval('root.%s' % attr)\n",
    "            if not all(elem in x for elem in [None, [], \"\", set(), False]):    # if not all elements in list are null\n",
    "                seq.extend(['(',attr])\n",
    "                #print('\\t'*(deep+1)+attr)\n",
    "                #deep += 1\n",
    "                for i in eval('root.%s' % attr):    # recursive the list\n",
    "                    if i is None or isinstance(i, str):    # perhaps it has None value in the list\n",
    "                        continue\n",
    "                    #deep += 1\n",
    "                    seq.extend(parse_tree(i, deep))\n",
    "                    \n",
    "                    #deep -= 1\n",
    "                #deep -= 1\n",
    "                seq.extend([')',attr])\n",
    "        elif 'tree' in str(type(eval('root.%s' % attr))):    #if the attr is one kind of Node, recursive the Node\n",
    "            seq.extend(['(',attr])\n",
    "            #print('\\t'*(deep+1)+attr)\n",
    "            #deep += 2\n",
    "            seq.extend(parse_tree(eval('root.%s' % attr), deep))\n",
    "            #deep -= 2\n",
    "            seq.extend([')',attr])\n",
    "        else:\n",
    "            seq.extend(['(','<'+str(attr)+'>_'+str(eval('root.%s' % attr)),')','<'+str(attr)+'>_'+str(eval('root.%s' % attr))])\n",
    "            #exec(\"print('\\t'*(deep+1)+attr+': '+str(root.%s))\" % attr)    #it must be normal attribute\n",
    "    seq.extend([')', str(root).split('(')[0]])\n",
    "    return seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Function:\n",
    "    1. \"camelCase\" -> [\"camel\", \"Case\"]\n",
    "    2. \"under_score\" -> [\"under\", \"_\", \"score\"]\n",
    "    3. \"normal\" -> [\"normal\"]\n",
    "'''\n",
    "def split_identifier(id_token):\n",
    "    if  \"_\" in id_token:\n",
    "        return id_token.split(\"_\")\n",
    "    elif id_token != id_token.lower() and id_token != id_token.upper():\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', id_token)\n",
    "        return [m.group(0) for m in matches]\n",
    "    else:\n",
    "        return [id_token]\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "Function:\n",
    "    1. input the list of train, test, valid dataset\n",
    "    2. filter the dataset, split it to train, test, valid set and save as the smaller dataset.\n",
    "    3. return the amount of the data from smaller datasets.\n",
    "Example:\n",
    "    filter_dataset(['./data/train.json', './data/test.json', './data/valid.json'], './data')\n",
    "Note:\n",
    "    The filter method is different from the method in DeepCom, because I have no idea how DeepCom did.\n",
    "    It doesn't make sense that DeepCom could filter so many data via the method mentioned in its paper.\n",
    "'''\n",
    "def filter_dataset(path_list, save_path):\n",
    "    \n",
    "    inputs = []\n",
    "    for path in path_list:\n",
    "        input_file = open(path)\n",
    "        inputs.extend(input_file.readlines())\n",
    "        input_file.close()\n",
    "    outputs = []\n",
    "    output_train_file = open(save_path+'/filter_train.json', \"w\")\n",
    "    output_test_file = open(save_path+'/filter_test.json', \"w\")\n",
    "    output_valid_file = open(save_path+'/filter_valid.json', \"w\")\n",
    "    \n",
    "    print('Original total: '+str(len(inputs)))\n",
    "    for pair in inputs:\n",
    "        pair = json.loads(pair)\n",
    "        tokens_parse = javalang.tokenizer.tokenize(pair['code'])\n",
    "        if is_invalid_method(len(list(tokens_parse)), pair['nl']):\n",
    "            continue\n",
    "        outputs.append(json.dumps(pair))\n",
    "\n",
    "    random.shuffle(outputs)\n",
    "    print('Final total: '+str(len(outputs)))\n",
    "    print('Data shuffle complete')\n",
    "    train_index = int(len(outputs)*0.8)\n",
    "    test_index = int(len(outputs)*0.9)\n",
    "    train_output = outputs[:train_index]\n",
    "    test_output = outputs[train_index+1:test_index]\n",
    "    valid_output = outputs[test_index+1:]\n",
    "    \n",
    "    for row in train_output:\n",
    "        output_train_file.write(row+'\\n')\n",
    "    output_train_file.close()\n",
    "    print('filter train data finish writing')\n",
    "    for row in test_output:\n",
    "        output_test_file.write(row+'\\n')\n",
    "    output_test_file.close()\n",
    "    print('filter test data finish writing')\n",
    "    for row in valid_output:\n",
    "        output_valid_file.write(row+'\\n')\n",
    "    print('filter valid data finish writing')\n",
    "    output_valid_file.close()\n",
    "\n",
    "    return len(train_output), len(test_output), len(valid_output)\n",
    "\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "    path: the path of the data you want to read\n",
    "    code_voc: code vocabulary, the data type is list\n",
    "    comment_voc: comment vocabulary, the data type is list\n",
    "Return values:\n",
    "    code_tokens, comment_tokens: 2-dimension list, store the code and comment into list, snippet by snippet\n",
    "    code_voc, comment_voc: the all vocabularies in the file of the path, data type is list\n",
    "Note:\n",
    "    It hasn't used SBT in DeepCom.\n",
    "TODO:\n",
    "    Change the rare words in comments into other common words via pre-trained embedding\n",
    "'''\n",
    "def readdata(path, code_voc, comment_voc):\n",
    "    input_file = open(path)\n",
    "    inputs = input_file.readlines()\n",
    "\n",
    "    code_tokens = []          # code_tokens = ['<START>', '<Modifier>', 'public', '<Identifier>',....]\n",
    "    comment_tokens = []       # comment_tokens = []\n",
    "\n",
    "    start = time.time()\n",
    "    for index, pair in enumerate(inputs):\n",
    "        if index%5000 == 0 and index != 0:\n",
    "            print(index)\n",
    "        pair = json.loads(pair)\n",
    "        # =============== extract the code part of the snippet =========================\n",
    "        \n",
    "        #TODO: 以下寫成另一個function，切token的部分\n",
    "        tokens_parse = javalang.tokenizer.tokenize(pair['code'])\n",
    "        tokens = []\n",
    "        for token in tokens_parse:    # iterate the tokens of the sentence\n",
    "            token = str(token).split(' ')\n",
    "            splitted_id = split_identifier(token[1].strip('\"'))    # split the camelCase and under_score\n",
    "            temp = ['<'+token[0]+'>']    # token[0] is token type, token[1] is token value\n",
    "            temp.extend(splitted_id)\n",
    "            tokens.extend(temp)\n",
    "            for x in tokens:\n",
    "                if x not in code_voc:\n",
    "                    code_voc.append(x)\n",
    "        tokens.insert(0, '<START>')\n",
    "        tokens.append('<END>')\n",
    "        code_tokens.append(tokens)\n",
    "        \n",
    "        #=============== extract comment part of the snippet ==========================\n",
    "        tokens = nltk.word_tokenize(pair['nl'])\n",
    "        tokens.append('<END>')\n",
    "        comment_tokens.append(tokens)\n",
    "        for x in tokens:\n",
    "            if x not in comment_voc:\n",
    "                comment_voc.append(x)\n",
    "\n",
    "    print('readdata:')\n",
    "    print('\\tdata amount: '+str(len(code_tokens)))\n",
    "    print('\\trun time: '+str(time.time()-start))\n",
    "\n",
    "    input_file.close()\n",
    "    return code_tokens, comment_tokens, code_voc, comment_voc\n",
    "\n",
    "def simple_readdata(path, code_voc, comment_voc):\n",
    "    input_file = open(path)\n",
    "    inputs = input_file.readlines()\n",
    "\n",
    "    code_tokens = []          \n",
    "    comment_tokens = []       # comment_tokens = []\n",
    "\n",
    "    start = time.time()\n",
    "    for index, pair in enumerate(inputs):\n",
    "        if index%5000 == 0 and index != 0:\n",
    "            print(index)\n",
    "        pair = json.loads(pair)\n",
    "        # =============== extract the code part of the snippet =========================\n",
    "        \n",
    "        #TODO: 以下寫成另一個function，切token的部分\n",
    "        tokens_parse = javalang.tokenizer.tokenize(pair['code'])\n",
    "        tokens = []\n",
    "        for token in tokens_parse:    # iterate the tokens of the sentence\n",
    "            token = str(token).split(' ')\n",
    "            splitted_id = split_identifier(token[1].strip('\"'))    # split the camelCase and under_score\n",
    "            #temp = ['<'+token[0]+'>']    # token[0] is token type, token[1] is token value\n",
    "            #temp = []\n",
    "            #temp.extend(splitted_id)\n",
    "            tokens.extend(splitted_id)\n",
    "            for x in tokens:\n",
    "                if x not in code_voc:\n",
    "                    code_voc.append(x)\n",
    "        tokens.insert(0, '<START>')\n",
    "        tokens.append('<END>')\n",
    "        code_tokens.append(tokens)\n",
    "        \n",
    "        #=============== extract comment part of the snippet ==========================\n",
    "        tokens = nltk.word_tokenize(pair['nl'])\n",
    "        tokens.append('<END>')\n",
    "        comment_tokens.append(tokens)\n",
    "        for x in tokens:\n",
    "            if x not in comment_voc:\n",
    "                comment_voc.append(x)\n",
    "\n",
    "    print('readdata:')\n",
    "    print('\\tdata amount: '+str(len(code_tokens)))\n",
    "    print('\\trun time: '+str(time.time()-start))\n",
    "\n",
    "    input_file.close()\n",
    "    return code_tokens, comment_tokens, code_voc, comment_voc\n",
    "\n",
    "'''\n",
    "Usage:\n",
    "    Transform the token to the index in vocabulary\n",
    "    ['<START>', '<Modifier>', 'public', ..., '<Separator>', ';', '<Separator>', '}', '<END>']\n",
    "    => [0, 7, 8, ..., 14, 29, 14, 30, 1]\n",
    "Parameter data type: \n",
    "    2-dimension list\n",
    "Return data type:\n",
    "    2-dimension list\n",
    "'''\n",
    "def token2index(lst, voc):\n",
    "    for index, seq in enumerate(lst):\n",
    "        seq_index = []\n",
    "        for token in seq:\n",
    "            seq_index.append(voc.index(token))\n",
    "        lst[index] = seq_index\n",
    "    return lst\n",
    "\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "    lst: the list of sequences to be padded\n",
    "    pad_data: the value you want to pad\n",
    "Return type:\n",
    "    numpy array\n",
    "'''\n",
    "def pad_sequences(lst, pad_data):\n",
    "    maxlen = max(len(x) for x in lst)\n",
    "    for index, seq in enumerate(lst):\n",
    "        lst[index].extend([pad_data] * (maxlen-len(seq)))\n",
    "    return np.array(lst)\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "    x: the list of data\n",
    "    batch_sz: batch size\n",
    "Return shape:\n",
    "    [None, batch_sz, None]\n",
    "Example:\n",
    "    a = [1,2,3,4,5,6,7,8,9,10]\n",
    "    a = getBatch(x=a, batch_sz=3)\n",
    "    a\n",
    "    ---output---\n",
    "    [[1,2,3], [4,5,6], [7,8,9]]\n",
    "'''\n",
    "def getBatch(x, batch_sz):\n",
    "    dataset = []\n",
    "    while(len(x)>=batch_sz):\n",
    "        dataset.append(x[:batch_sz])\n",
    "        x = x[batch_sz:]\n",
    "    if type(x) == np.ndarray:\n",
    "        return np.array(dataset)\n",
    "    elif type(x) == list:\n",
    "        return dataset\n",
    "    \n",
    "def ngram(words, n):\n",
    "    return list(zip(*(words[i:] for i in range(n))))\n",
    "\n",
    "def bleu4(true, pred):\n",
    "    true = nltk.word_tokenize(true)\n",
    "    pred = nltk.word_tokenize(pred)\n",
    "    c = len(pred)\n",
    "    r = len(true)\n",
    "    bp = 1. if c > r else np.exp(1 - r / (c + 1e-10))\n",
    "    score = 0\n",
    "    for i in range(1, 5):\n",
    "        true_ngram = set(ngram(true, i))\n",
    "        pred_ngram = ngram(pred, i)\n",
    "        \n",
    "        length = float(len(pred_ngram)) + 1e-10\n",
    "        count = sum([1. if t in true_ngram else 0. for t in pred_ngram])\n",
    "        score += math.log(1e-10 + (count / length))\n",
    "    score = math.exp(score * .25)\n",
    "    bleu = bp * score\n",
    "    return bleu\n",
    "\n",
    "# TODO: 必須解決test set有可能有<UNK>，不然沒辦法translate test set\n",
    "def evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    tokens_parse = javalang.tokenizer.tokenize(code)\n",
    "    inputs = []\n",
    "    for token in tokens_parse:    # iterate the tokens of the sentence\n",
    "        token = str(token).split(' ')\n",
    "        splitted_id = split_identifier(token[1].strip('\"'))    # split the camelCase and under_score\n",
    "        temp = ['<'+token[0]+'>']    # token[0] is token type, token[1] is token value\n",
    "        temp.extend(splitted_id)\n",
    "        inputs.extend(temp)\n",
    "\n",
    "    inputs.insert(0, '<START>')\n",
    "    inputs.append('<END>')\n",
    "    inputs += ['<PAD>'] * (max_length_inp - len(inputs))\n",
    "    for index, token in enumerate(inputs):\n",
    "        if token not in code_voc:\n",
    "            inputs[index] = code_voc.index('<UNK>')\n",
    "        else:\n",
    "            inputs[index] = code_voc.index(token)\n",
    "    inputs = np.array(inputs)\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    #hidden = [tf.zeros((1, units))]\n",
    "    #enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    #dec_hidden = enc_hidden\n",
    "    \n",
    "    hidden_h, hidden_c = tf.zeros((1, units)), tf.zeros((1, units))\n",
    "    hidden = [hidden_h, hidden_c]\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inputs, hidden)\n",
    "    dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "    \n",
    "    dec_input = tf.expand_dims([comment_voc.index('<START>')], 1)       \n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_h, dec_hidden_c, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "        dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "        #predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if comment_voc[predicted_id] == '<END>':\n",
    "            return result, code, attention_plot\n",
    "        \n",
    "        result += comment_voc[predicted_id] + ' '\n",
    "        \n",
    "    \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, code, attention_plot\n",
    "\n",
    "def simple_evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    tokens_parse = javalang.tokenizer.tokenize(code)\n",
    "    inputs = []\n",
    "    for token in tokens_parse:    # iterate the tokens of the sentence\n",
    "        token = str(token).split(' ')\n",
    "        splitted_id = split_identifier(token[1].strip('\"'))    # split the camelCase and under_score\n",
    "        inputs.extend(splitted_id)\n",
    "\n",
    "    inputs.insert(0, '<START>')\n",
    "    inputs.append('<END>')\n",
    "    inputs += ['<PAD>'] * (max_length_inp - len(inputs))\n",
    "    for index, token in enumerate(inputs):\n",
    "        if token not in code_voc:\n",
    "            inputs[index] = code_voc.index('<UNK>')\n",
    "        else:\n",
    "            inputs[index] = code_voc.index(token)\n",
    "    inputs = np.array(inputs)\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    #hidden = [tf.zeros((1, units))]\n",
    "    #enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    #dec_hidden = enc_hidden\n",
    "    \n",
    "    hidden_h, hidden_c = tf.zeros((1, units)), tf.zeros((1, units))\n",
    "    hidden = [hidden_h, hidden_c]\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inputs, hidden)\n",
    "    dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "    \n",
    "    dec_input = tf.expand_dims([comment_voc.index('<START>')], 1)       \n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_h, dec_hidden_c, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "        dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "        #predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if comment_voc[predicted_id] == '<END>':\n",
    "            return result, code, attention_plot\n",
    "        \n",
    "        result += comment_voc[predicted_id] + ' '\n",
    "    \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, code, attention_plot\n",
    "\n",
    "\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def translate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ):\n",
    "    result, code, attention_plot = evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ)\n",
    "\n",
    "def simple_translate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ):\n",
    "    result, code, attention_plot = simple_evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ)\n",
    "\n",
    "    #attention_plot = attention_plot[:len(result.split(' ')), :len(code.split(' '))]\n",
    "    #plot_attention(attention_plot, code.split(' '), result.split(' '))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the original big dataset into simpler and better dataset\n",
    "* #### Size of training set, testing set and valid set ->  (81932, 10241, 10241)\n",
    "* #### If you already have \"filter_train.json\", \"filter_test.json\" and \"filter_valid.json\", then you can skip this code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original total: 588108\n",
      "Final total: 101220\n",
      "Data shuffle complete\n",
      "filter train data finish writing\n",
      "filter test data finish writing\n",
      "filter valid data finish writing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80976, 10121, 10121)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_dataset(['./data/train.json', './data/test.json', './data/valid.json'], './filter_dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading training data (it costs about 40 mins)\n",
    "* #### If you already have 'train_data.pkl', you can skip this code cell below and directly read 'train_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "readdata:\n",
      "\tdata amount: 80976\n",
      "\trun time: 2137.429878473282\n",
      "size of code vocabulary:  42137\n",
      "size of comment vocabulary:  29404\n"
     ]
    }
   ],
   "source": [
    "code_voc = ['<PAD>','<START>','<END>','<UNK>']\n",
    "comment_voc = ['<PAD>','<START>','<END>','<UNK>']\n",
    "code_train, comment_train, code_voc, comment_voc = simple_readdata('./filter_dataset/filter_train.json', code_voc, comment_voc)\n",
    "#code_train, comment_train, code_voc, comment_voc = readdata('./filter_dataset/filter_train.json', code_voc, comment_voc)\n",
    "#code_test, comment_test, code_voc, comment_voc = readdata('./filter_dataset/filter_test.json', code_voc, comment_voc)\n",
    "\n",
    "code_train = token2index(code_train, code_voc)\n",
    "comment_train = token2index(comment_train, comment_voc)\n",
    "code_train = pad_sequences(code_train, code_voc.index('<PAD>'))\n",
    "comment_train = pad_sequences(comment_train, comment_voc.index('<PAD>'))\n",
    "print('size of code vocabulary: ', len(code_voc))\n",
    "print('size of comment vocabulary: ', len(comment_voc))\n",
    "\n",
    "# Saving the training data:\n",
    "with open('train_simple_data.pkl', 'wb') as f:\n",
    "    pickle.dump([code_train, comment_train, code_voc, comment_voc], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of code vocabulary:  42137\n",
      "size of comment vocabulary:  29404\n"
     ]
    }
   ],
   "source": [
    "# Getting back the training data:\n",
    "with open('train_simple_data.pkl', 'rb') as f:\n",
    "    code_train, comment_train, code_voc, comment_voc = pickle.load(f)\n",
    "    \n",
    "print('size of code vocabulary: ', len(code_voc))\n",
    "print('size of comment vocabulary: ', len(comment_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just test the functionality of transforming source code to SBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open('./data/test.json')\n",
    "inputs = input_file.readlines()\n",
    "pair = json.loads(inputs[0])\n",
    "tree = javalang.parse.parse('class aa {'+pair['code']+'}')\n",
    "print(pair['code'], end='\\n')\n",
    "\n",
    "_, node = list(tree)[2]    # 前兩個用來篩掉class aa{ }的部分\n",
    "seq = parse_tree(node, 0)\n",
    "for i in seq:\n",
    "    print(i,end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyper-parameters and some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(code_train)\n",
    "BATCH_SIZE = 32\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 256\n",
    "vocab_inp_size = len(code_voc)\n",
    "vocab_tar_size = len(comment_voc)\n",
    "\n",
    "max_length_inp = max(len(t) for t in code_train)\n",
    "max_length_targ = max(len(t) for t in comment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep learing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(units):\n",
    "    return tf.keras.layers.LSTM(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = lstm(self.enc_units)\n",
    "        #self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = hidden)        \n",
    "        #output, state = self.gru(x, initial_state = hidden) \n",
    "        return output, state_h, state_c\n",
    "        #return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))\n",
    "    #def initialize_hidden_state(self):\n",
    "    #    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = lstm(self.dec_units)\n",
    "        #self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden[1], 1)\n",
    "        #hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the LSTM\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        #output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state_h, state_c, attention_weights\n",
    "        #return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units)), tf.zeros((self.batch_sz, self.dec_units))\n",
    "    #def initialize_hidden_state(self):\n",
    "    #    return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the encoder, decoder and define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3)  #tensorflow 2.0\n",
    "#optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the checkpoint object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/adam-simple-256-50epochs'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "lossArray = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read english and french data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossArray = np.array([])\n",
    "from sklearn.model_selection import train_test_split\n",
    "f = open('./en.txt', 'r', encoding='utf-8')\n",
    "en_lines = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('./fr.txt', 'r', encoding='utf-8-sig')\n",
    "fr_lines = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "en_sent = []\n",
    "en_words = ['<PAD>','<START>','<END>','<UNK>']\n",
    "fr_sent = []\n",
    "fr_words = ['<PAD>','<START>','<END>','<UNK>']\n",
    "\n",
    "\n",
    "# 把每句英法文做tokenize，存到en_sent, fr_sent。\n",
    "# 把所有出現過的英法單字存到，en_words, fr_words。\n",
    "for line in en_lines:\n",
    "    words = nltk.word_tokenize(line)\n",
    "    en_sent.append(words)\n",
    "    for x in words:\n",
    "        if x not in en_words:\n",
    "            en_words.append(x)\n",
    "            \n",
    "for line in fr_lines:\n",
    "    words = nltk.word_tokenize(line)\n",
    "    fr_sent.append(words)\n",
    "    for x in words:\n",
    "        if x not in fr_words:\n",
    "            fr_words.append(x)\n",
    "\n",
    "\n",
    "#替句子前後加上標籤\n",
    "for sent in en_sent:\n",
    "    sent.insert(0, '<START>')\n",
    "    sent.append('<END>')\n",
    "for sent in fr_sent:\n",
    "    sent.insert(0, '<START>')\n",
    "    sent.append('<END>')\n",
    "\n",
    "# 製作word, index對應字典\n",
    "#en_word_index = dict(\n",
    "#    [(word, i+4) for i, word in enumerate(en_words)])\n",
    "#fr_word_index = dict(\n",
    "#    [(word, i+4) for i, word in enumerate(fr_words)])\n",
    "\n",
    "#en_word_index_rev = dict(\n",
    "#    [(i+4, word) for i, word in enumerate(en_words)])\n",
    "#fr_word_index_rev = dict(\n",
    "#    [(i+4, word) for i, word in enumerate(fr_words)])\n",
    "\n",
    "\n",
    "#en_word_index['<PAD>'] = 0\n",
    "#en_word_index['<START>'] = 1\n",
    "#en_word_index['<END>'] = 2\n",
    "#en_word_index['<UNK>'] = 3\n",
    "\n",
    "#fr_word_index['<PAD>'] = 0\n",
    "#fr_word_index['<START>'] = 1\n",
    "#fr_word_index['<END>'] = 2\n",
    "#fr_word_index['<UNK>'] = 3\n",
    "    \n",
    "\n",
    "# 英法文單字種類數\n",
    "#num_en_words = len(en_word_index)\n",
    "#num_fr_words = len(fr_word_index)\n",
    "# 英法句子最長長度\n",
    "max_len_en = max([len(x) for x in en_sent])\n",
    "max_len_fr = max([len(x) for x in fr_sent])\n",
    "\n",
    "\n",
    "# 把sentence從word換成index形式，e.g. ['I','like','banana'] => [56,35,71]\n",
    "for sent in en_sent:\n",
    "    for i, token in enumerate(sent):\n",
    "        sent[i] = en_words.index(token)\n",
    "\n",
    "for sent in fr_sent:\n",
    "    for i, token in enumerate(sent):\n",
    "        sent[i] = fr_words.index(token)\n",
    "        \n",
    "\n",
    "en_sent = tf.keras.preprocessing.sequence.pad_sequences(en_sent,\n",
    "                                                        value=en_words.index(\"<PAD>\"),\n",
    "                                                        padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                        maxlen=max_len_en)\n",
    "\n",
    "fr_sent = tf.keras.preprocessing.sequence.pad_sequences(fr_sent,\n",
    "                                                        value=fr_words.index(\"<PAD>\"), \n",
    "                                                        padding='post', \n",
    "                                                        truncating='post', \n",
    "                                                        maxlen=max_len_fr)\n",
    "\n",
    "en_train, en_test, fr_train, fr_test = train_test_split(en_sent, fr_sent, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# decoder的input結尾去掉<END>\n",
    "# decoder的output開頭去掉<START>\n",
    "fr_output_train = fr_train[:,1:]\n",
    "fr_output_test = fr_test[:,1:]\n",
    "\n",
    "fr_input_train = fr_train\n",
    "fr_input_train = np.where(fr_input_train==fr_words.index('<END>'), fr_words.index('<PAD>'), fr_input_train)\n",
    "fr_input_train = np.delete(fr_input_train, 26, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En to fr version: build the model and optimizer and define evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(en_train)\n",
    "BATCH_SIZE = 32\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 256\n",
    "\n",
    "encoder = Encoder(len(en_words), embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(len(fr_words), embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3)  #tensorflow 2.0\n",
    "#optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    #tokens_parse = javalang.tokenizer.tokenize(code)\n",
    "    inputs = nltk.word_tokenize(code)\n",
    "    #inputs = []\n",
    "\n",
    "    inputs.insert(0, '<START>')\n",
    "    inputs.append('<END>')\n",
    "    inputs += ['<PAD>'] * (max_length_inp - len(inputs))\n",
    "    for index, token in enumerate(inputs):\n",
    "        if token not in code_voc:\n",
    "            inputs[index] = code_voc.index('<UNK>')\n",
    "        else:\n",
    "            inputs[index] = code_voc.index(token)\n",
    "    inputs = np.array(inputs)\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    \n",
    "    #hidden = [tf.zeros((1, units))]\n",
    "    #enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    #dec_hidden = enc_hidden\n",
    "    \n",
    "    hidden_h, hidden_c = tf.zeros((1, units)), tf.zeros((1, units))\n",
    "    hidden = [hidden_h, hidden_c]\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inputs, hidden)\n",
    "    dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "    \n",
    "    \n",
    "    dec_input = tf.expand_dims([comment_voc.index('<START>')], 1)       \n",
    "    \n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_h, dec_hidden_c, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "        dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "        #predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += comment_voc[predicted_id] + ' '\n",
    "\n",
    "        if comment_voc[predicted_id] == '<END>':\n",
    "            return result, code, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, code, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train en to fr translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden_h, hidden_c = encoder.initialize_hidden_state()\n",
    "    \n",
    "    hidden = [hidden_h, hidden_c]\n",
    "    #hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "\n",
    "    code_train_batch = getBatch(en_train, BATCH_SIZE)\n",
    "    \n",
    "    comment_train_batch = getBatch(fr_output_train, BATCH_SIZE)\n",
    "    \n",
    "    dataset = [(code_train_batch[i], comment_train_batch[i]) for i in range(0, len(code_train_batch))]\n",
    "    \n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, hidden)\n",
    "            #enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "            #dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([fr_words.index('<START>')] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "                #predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 200 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    \n",
    "    lossArray = np.append(lossArray, (total_loss / N_BATCH) )\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    # ============ can remove below ===============\n",
    "    index = 5\n",
    "    print(en_lines[index], end=\"\\n\\n\")\n",
    "    print(\"Original comment: \",fr_lines[index], end=\"\\n\")\n",
    "    predict = translate(en_lines[index], encoder, decoder, en_words, fr_words, max_len_en, max_len_fr)\n",
    "    print(\"Predictedd translation: \", predict, end=\"\\n\\n\")\n",
    "#    bleu4_score = bleu4(test_outputs[index], predict)\n",
    "#    print(\"bleu4: \", bleu4_score, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "print(en_lines[index], end=\"\\n\\n\")\n",
    "print(\"Original comment: \",fr_lines[index], end=\"\\n\")\n",
    "predict = translate(en_lines[index], encoder, decoder, en_words, fr_words, max_len_en, max_len_fr)\n",
    "print(\"Predictedd translation: \", predict, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./filter_dataset/filter_train.json')\n",
    "inputs = f.readlines()\n",
    "f.close()\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "for pair in inputs[:100]:\n",
    "    pair = json.loads(pair)\n",
    "    test_inputs.append(pair['code'])\n",
    "    test_outputs.append(pair['nl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.8004\n",
      "Epoch 1 Batch 200 Loss 1.9764\n",
      "Epoch 1 Batch 400 Loss 1.7707\n",
      "Epoch 1 Batch 600 Loss 1.8508\n",
      "Epoch 1 Batch 800 Loss 1.7922\n",
      "Epoch 1 Batch 1000 Loss 1.3299\n",
      "Epoch 1 Batch 1200 Loss 1.4427\n",
      "Epoch 1 Batch 1400 Loss 1.5963\n",
      "Epoch 1 Batch 1600 Loss 1.1746\n",
      "Epoch 1 Batch 1800 Loss 1.5477\n",
      "Epoch 1 Batch 2000 Loss 1.4988\n",
      "Epoch 1 Batch 2200 Loss 1.7582\n",
      "Epoch 1 Batch 2400 Loss 1.8404\n",
      "Epoch 1 Loss 1.6109\n",
      "Time taken for 1 epoch 1634.6127271652222 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Creates a new instance of the given file \n",
      "bleu4:  1.0000000000124996e-10\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0355\n",
      "Epoch 2 Batch 200 Loss 1.3063\n",
      "Epoch 2 Batch 400 Loss 1.2184\n",
      "Epoch 2 Batch 600 Loss 1.2146\n",
      "Epoch 2 Batch 800 Loss 1.3875\n",
      "Epoch 2 Batch 1000 Loss 1.2802\n",
      "Epoch 2 Batch 1200 Loss 0.9926\n",
      "Epoch 2 Batch 1400 Loss 1.3942\n",
      "Epoch 2 Batch 1600 Loss 1.4066\n",
      "Epoch 2 Batch 2000 Loss 1.1664\n",
      "Epoch 2 Batch 2200 Loss 1.1933\n",
      "Epoch 2 Batch 2400 Loss 1.4193\n",
      "Epoch 2 Loss 1.3185\n",
      "Time taken for 1 epoch 1638.0951507091522 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Add a new instance of the given name \n",
      "bleu4:  1.0000000000124996e-10\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0606\n",
      "Epoch 3 Batch 200 Loss 1.3254\n",
      "Epoch 3 Batch 400 Loss 1.2602\n",
      "Epoch 3 Batch 600 Loss 1.1203\n",
      "Epoch 3 Batch 800 Loss 1.1128\n",
      "Epoch 3 Batch 1000 Loss 1.2624\n",
      "Epoch 3 Batch 1200 Loss 1.2485\n",
      "Epoch 3 Batch 1400 Loss 1.0435\n",
      "Epoch 3 Batch 1600 Loss 1.2752\n",
      "Epoch 3 Batch 1800 Loss 1.0008\n",
      "Epoch 3 Batch 2000 Loss 1.2147\n",
      "Epoch 3 Batch 2200 Loss 1.1629\n",
      "Epoch 3 Batch 2400 Loss 1.3357\n",
      "Epoch 3 Loss 1.1790\n",
      "Time taken for 1 epoch 1632.7970685958862 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Add a new instance of the given name \n",
      "bleu4:  1.0000000000124996e-10\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3546\n",
      "Epoch 4 Batch 200 Loss 1.0021\n",
      "Epoch 4 Batch 400 Loss 1.1504\n",
      "Epoch 4 Batch 600 Loss 1.3811\n",
      "Epoch 4 Batch 800 Loss 1.0852\n",
      "Epoch 4 Batch 1000 Loss 0.8041\n",
      "Epoch 4 Batch 1200 Loss 1.1496\n",
      "Epoch 4 Batch 1400 Loss 0.9965\n",
      "Epoch 4 Batch 1600 Loss 0.8973\n",
      "Epoch 4 Batch 1800 Loss 1.0627\n",
      "Epoch 4 Batch 2000 Loss 1.0039\n",
      "Epoch 4 Batch 2200 Loss 1.0166\n",
      "Epoch 4 Batch 2400 Loss 1.0657\n",
      "Epoch 4 Loss 1.0732\n",
      "Time taken for 1 epoch 1635.1191985607147 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Adds a new instance of the given tenant 's activity \n",
      "bleu4:  9.999999999999996e-11\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2091\n",
      "Epoch 5 Batch 200 Loss 1.0931\n",
      "Epoch 5 Batch 400 Loss 0.9417\n",
      "Epoch 5 Batch 600 Loss 1.2574\n",
      "Epoch 5 Batch 800 Loss 0.9688\n",
      "Epoch 5 Batch 1000 Loss 1.0392\n",
      "Epoch 5 Batch 1200 Loss 0.9250\n",
      "Epoch 5 Batch 1400 Loss 0.8876\n",
      "Epoch 5 Batch 1600 Loss 0.8414\n",
      "Epoch 5 Batch 1800 Loss 1.0591\n",
      "Epoch 5 Batch 2000 Loss 0.9587\n",
      "Epoch 5 Batch 2200 Loss 0.9029\n",
      "Epoch 5 Batch 2400 Loss 0.7641\n",
      "Epoch 5 Loss 0.9842\n",
      "Time taken for 1 epoch 1633.3722231388092 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Send the crop Intent with the specified node \n",
      "bleu4:  1.0000000000124996e-10\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.0305\n",
      "Epoch 6 Batch 200 Loss 0.9351\n",
      "Epoch 6 Batch 400 Loss 1.0829\n",
      "Epoch 6 Batch 600 Loss 0.9235\n",
      "Epoch 6 Batch 800 Loss 0.9339\n",
      "Epoch 6 Batch 1000 Loss 0.9606\n",
      "Epoch 6 Batch 1200 Loss 0.8868\n",
      "Epoch 6 Batch 1400 Loss 1.0175\n",
      "Epoch 6 Batch 1600 Loss 0.9807\n",
      "Epoch 6 Batch 1800 Loss 0.9604\n",
      "Epoch 6 Batch 2000 Loss 0.9420\n",
      "Epoch 6 Batch 2200 Loss 0.8886\n",
      "Epoch 6 Batch 2400 Loss 0.9714\n",
      "Epoch 6 Loss 0.9097\n",
      "Time taken for 1 epoch 1631.2953476905823 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Send a listener to the current thread \n",
      "bleu4:  8.668778997643345e-11\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0870\n",
      "Epoch 7 Batch 200 Loss 0.7580\n",
      "Epoch 7 Batch 400 Loss 0.8306\n",
      "Epoch 7 Batch 600 Loss 0.8631\n",
      "Epoch 7 Batch 800 Loss 0.8898\n",
      "Epoch 7 Batch 1000 Loss 0.7702\n",
      "Epoch 7 Batch 1200 Loss 0.9397\n",
      "Epoch 7 Batch 1400 Loss 0.8656\n",
      "Epoch 7 Batch 1600 Loss 0.8998\n",
      "Epoch 7 Batch 1800 Loss 0.8124\n",
      "Epoch 7 Batch 2000 Loss 0.8754\n",
      "Epoch 7 Batch 2200 Loss 0.9461\n",
      "Epoch 7 Batch 2400 Loss 0.6820\n",
      "Epoch 7 Loss 0.8457\n",
      "Time taken for 1 epoch 1630.8448059558868 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Send the XpressNet message to add a listener for a new instance \n",
      "bleu4:  9.999999999999996e-11\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.6802\n",
      "Epoch 8 Batch 200 Loss 0.5594\n",
      "Epoch 8 Batch 400 Loss 0.6871\n",
      "Epoch 8 Batch 600 Loss 0.7880\n",
      "Epoch 8 Batch 800 Loss 1.0578\n",
      "Epoch 8 Batch 1000 Loss 0.7944\n",
      "Epoch 8 Batch 1200 Loss 0.7152\n",
      "Epoch 8 Batch 1400 Loss 0.8031\n",
      "Epoch 8 Batch 1600 Loss 0.6864\n",
      "Epoch 8 Batch 1800 Loss 0.6953\n",
      "Epoch 8 Batch 2000 Loss 0.8974\n",
      "Epoch 8 Batch 2200 Loss 0.7452\n",
      "Epoch 8 Batch 2400 Loss 0.7926\n",
      "Epoch 8 Loss 0.7902\n",
      "Time taken for 1 epoch 1635.329342365265 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Send the XpressNet message to add the given listener \n",
      "bleu4:  9.999999999999996e-11\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.9292\n",
      "Epoch 9 Batch 200 Loss 0.7311\n",
      "Epoch 9 Batch 400 Loss 0.6292\n",
      "Epoch 9 Batch 600 Loss 0.9282\n",
      "Epoch 9 Batch 800 Loss 0.7474\n",
      "Epoch 9 Batch 1000 Loss 0.7355\n",
      "Epoch 9 Batch 1200 Loss 0.7906\n",
      "Epoch 9 Batch 1400 Loss 0.5720\n",
      "Epoch 9 Batch 1600 Loss 0.6725\n",
      "Epoch 9 Batch 1800 Loss 0.7618\n",
      "Epoch 9 Batch 2000 Loss 0.7160\n",
      "Epoch 9 Batch 2200 Loss 0.7868\n",
      "Epoch 9 Batch 2400 Loss 0.6023\n",
      "Epoch 9 Loss 0.7488\n",
      "Time taken for 1 epoch 1634.6734261512756 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a rational number from the server \n",
      "bleu4:  1.685324061307668e-08\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.7711\n",
      "Epoch 10 Batch 200 Loss 0.7592\n",
      "Epoch 10 Batch 400 Loss 0.7110\n",
      "Epoch 10 Batch 600 Loss 0.7742\n",
      "Epoch 10 Batch 800 Loss 0.5624\n",
      "Epoch 10 Batch 1000 Loss 0.6043\n",
      "Epoch 10 Batch 1200 Loss 0.5226\n",
      "Epoch 10 Batch 1400 Loss 0.7444\n",
      "Epoch 10 Batch 1600 Loss 0.8619\n",
      "Epoch 10 Batch 1800 Loss 0.7640\n",
      "Epoch 10 Batch 2000 Loss 0.5884\n",
      "Epoch 10 Batch 2200 Loss 0.6192\n",
      "Epoch 10 Batch 2400 Loss 0.8858\n",
      "Epoch 10 Loss 0.7021\n",
      "Time taken for 1 epoch 1632.2001786231995 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.7293\n",
      "Epoch 11 Batch 200 Loss 0.6531\n",
      "Epoch 11 Batch 400 Loss 0.5386\n",
      "Epoch 11 Batch 600 Loss 0.5962\n",
      "Epoch 11 Batch 800 Loss 0.6611\n",
      "Epoch 11 Batch 1000 Loss 0.6415\n",
      "Epoch 11 Batch 1200 Loss 0.6112\n",
      "Epoch 11 Batch 1400 Loss 0.6053\n",
      "Epoch 11 Batch 1600 Loss 0.8005\n",
      "Epoch 11 Batch 1800 Loss 0.5297\n",
      "Epoch 11 Batch 2000 Loss 0.6992\n",
      "Epoch 11 Batch 2200 Loss 0.5857\n",
      "Epoch 11 Batch 2400 Loss 0.6601\n",
      "Epoch 11 Loss 0.6620\n",
      "Time taken for 1 epoch 1632.377207517624 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.5871\n",
      "Epoch 12 Batch 200 Loss 0.5587\n",
      "Epoch 12 Batch 400 Loss 0.6650\n",
      "Epoch 12 Batch 600 Loss 0.6056\n",
      "Epoch 12 Batch 800 Loss 0.8004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 1000 Loss 0.6514\n",
      "Epoch 12 Batch 1200 Loss 0.5853\n",
      "Epoch 12 Batch 1400 Loss 0.6543\n",
      "Epoch 12 Batch 1600 Loss 0.7689\n",
      "Epoch 12 Batch 1800 Loss 0.5984\n",
      "Epoch 12 Batch 2000 Loss 0.6118\n",
      "Epoch 12 Batch 2200 Loss 0.7231\n",
      "Epoch 12 Batch 2400 Loss 0.6997\n",
      "Epoch 12 Loss 0.6268\n",
      "Time taken for 1 epoch 1633.4625051021576 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.5891\n",
      "Epoch 13 Batch 200 Loss 0.7118\n",
      "Epoch 13 Batch 400 Loss 0.4644\n",
      "Epoch 13 Batch 600 Loss 0.3725\n",
      "Epoch 13 Batch 800 Loss 0.4642\n",
      "Epoch 13 Batch 1000 Loss 0.6362\n",
      "Epoch 13 Batch 1200 Loss 0.6085\n",
      "Epoch 13 Batch 1400 Loss 0.5672\n",
      "Epoch 13 Batch 1600 Loss 0.5527\n",
      "Epoch 13 Batch 1800 Loss 0.4976\n",
      "Epoch 13 Batch 2000 Loss 0.7960\n",
      "Epoch 13 Batch 2200 Loss 0.6010\n",
      "Epoch 13 Batch 2400 Loss 0.6445\n",
      "Epoch 13 Loss 0.5941\n",
      "Time taken for 1 epoch 1637.0767352581024 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server on the server on the server on the server on the server on the server on the server on the server on the server on \n",
      "bleu4:  1.340169027790425e-08\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.5110\n",
      "Epoch 14 Batch 200 Loss 0.5035\n",
      "Epoch 14 Batch 400 Loss 0.5982\n",
      "Epoch 14 Batch 600 Loss 0.5839\n",
      "Epoch 14 Batch 800 Loss 0.7038\n",
      "Epoch 14 Batch 1000 Loss 0.5209\n",
      "Epoch 14 Batch 1200 Loss 0.5459\n",
      "Epoch 14 Batch 1400 Loss 0.5592\n",
      "Epoch 14 Batch 1600 Loss 0.6069\n",
      "Epoch 14 Batch 1800 Loss 0.6036\n",
      "Epoch 14 Batch 2000 Loss 0.5077\n",
      "Epoch 14 Batch 2200 Loss 0.5989\n",
      "Epoch 14 Batch 2400 Loss 0.5395\n",
      "Epoch 14 Loss 0.5630\n",
      "Time taken for 1 epoch 1631.5635557174683 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.5155\n",
      "Epoch 15 Batch 200 Loss 0.6557\n",
      "Epoch 15 Batch 400 Loss 0.4772\n",
      "Epoch 15 Batch 600 Loss 0.5312\n",
      "Epoch 15 Batch 800 Loss 0.7203\n",
      "Epoch 15 Batch 1000 Loss 0.4268\n",
      "Epoch 15 Batch 1200 Loss 0.5736\n",
      "Epoch 15 Batch 1400 Loss 0.3837\n",
      "Epoch 15 Batch 1600 Loss 0.4991\n",
      "Epoch 15 Batch 1800 Loss 0.4899\n",
      "Epoch 15 Batch 2000 Loss 0.6555\n",
      "Epoch 15 Batch 2200 Loss 0.3761\n",
      "Epoch 15 Batch 2400 Loss 0.5420\n",
      "Epoch 15 Loss 0.5356\n",
      "Time taken for 1 epoch 1631.8354370594025 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5045\n",
      "Epoch 16 Batch 200 Loss 0.5063\n",
      "Epoch 16 Batch 400 Loss 0.4048\n",
      "Epoch 16 Batch 600 Loss 0.4396\n",
      "Epoch 16 Batch 800 Loss 0.5490\n",
      "Epoch 16 Batch 1000 Loss 0.4846\n",
      "Epoch 16 Batch 1200 Loss 0.5683\n",
      "Epoch 16 Batch 1400 Loss 0.4206\n",
      "Epoch 16 Batch 1600 Loss 0.6403\n",
      "Epoch 16 Batch 1800 Loss 0.5515\n",
      "Epoch 16 Batch 2000 Loss 0.6025\n",
      "Epoch 16 Batch 2200 Loss 0.4905\n",
      "Epoch 16 Batch 2400 Loss 0.6661\n",
      "Epoch 16 Loss 0.5110\n",
      "Time taken for 1 epoch 1634.406106710434 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5683\n",
      "Epoch 17 Batch 200 Loss 0.3911\n",
      "Epoch 17 Batch 400 Loss 0.4016\n",
      "Epoch 17 Batch 600 Loss 0.6076\n",
      "Epoch 17 Batch 800 Loss 0.4394\n",
      "Epoch 17 Batch 1000 Loss 0.4051\n",
      "Epoch 17 Batch 1200 Loss 0.5037\n",
      "Epoch 17 Batch 1400 Loss 0.4868\n",
      "Epoch 17 Batch 1600 Loss 0.4362\n",
      "Epoch 17 Batch 1800 Loss 0.6089\n",
      "Epoch 17 Batch 2000 Loss 0.3943\n",
      "Epoch 17 Batch 2200 Loss 0.5256\n",
      "Epoch 17 Batch 2400 Loss 0.3595\n",
      "Epoch 17 Loss 0.4847\n",
      "Time taken for 1 epoch 1636.6952900886536 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.4289\n",
      "Epoch 18 Batch 200 Loss 0.5107\n",
      "Epoch 18 Batch 400 Loss 0.5572\n",
      "Epoch 18 Batch 600 Loss 0.4163\n",
      "Epoch 18 Batch 800 Loss 0.5079\n",
      "Epoch 18 Batch 1000 Loss 0.4972\n",
      "Epoch 18 Batch 1200 Loss 0.4698\n",
      "Epoch 18 Batch 1400 Loss 0.4025\n",
      "Epoch 18 Batch 1600 Loss 0.5269\n",
      "Epoch 18 Batch 1800 Loss 0.6066\n",
      "Epoch 18 Batch 2000 Loss 0.3218\n",
      "Epoch 18 Batch 2200 Loss 0.7152\n",
      "Epoch 18 Batch 2400 Loss 0.5276\n",
      "Epoch 18 Loss 0.4628\n",
      "Time taken for 1 epoch 1632.6687679290771 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.4192\n",
      "Epoch 19 Batch 200 Loss 0.3445\n",
      "Epoch 19 Batch 400 Loss 0.4885\n",
      "Epoch 19 Batch 600 Loss 0.3058\n",
      "Epoch 19 Batch 800 Loss 0.3940\n",
      "Epoch 19 Batch 1000 Loss 0.5523\n",
      "Epoch 19 Batch 1200 Loss 0.3108\n",
      "Epoch 19 Batch 1400 Loss 0.4183\n",
      "Epoch 19 Batch 1600 Loss 0.3684\n",
      "Epoch 19 Batch 1800 Loss 0.3181\n",
      "Epoch 19 Batch 2000 Loss 0.3304\n",
      "Epoch 19 Batch 2200 Loss 0.4427\n",
      "Epoch 19 Batch 2400 Loss 0.4649\n",
      "Epoch 19 Loss 0.4427\n",
      "Time taken for 1 epoch 1632.3776144981384 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an error to the server \n",
      "bleu4:  3.6409302405825886e-06\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.3463\n",
      "Epoch 20 Batch 200 Loss 0.3832\n",
      "Epoch 20 Batch 400 Loss 0.3486\n",
      "Epoch 20 Batch 600 Loss 0.2607\n",
      "Epoch 20 Batch 800 Loss 0.4678\n",
      "Epoch 20 Batch 1000 Loss 0.3639\n",
      "Epoch 20 Batch 1200 Loss 0.4558\n",
      "Epoch 20 Batch 1400 Loss 0.5012\n",
      "Epoch 20 Batch 1600 Loss 0.1888\n",
      "Epoch 20 Batch 1800 Loss 0.5229\n",
      "Epoch 20 Batch 2000 Loss 0.3446\n",
      "Epoch 20 Batch 2200 Loss 0.3278\n",
      "Epoch 20 Batch 2400 Loss 0.5984\n",
      "Epoch 20 Loss 0.4211\n",
      "Time taken for 1 epoch 1631.573778629303 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an event \n",
      "bleu4:  1.4351442320726098e-06\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.4624\n",
      "Epoch 21 Batch 200 Loss 0.3131\n",
      "Epoch 21 Batch 400 Loss 0.2388\n",
      "Epoch 21 Batch 600 Loss 0.3752\n",
      "Epoch 21 Batch 800 Loss 0.3426\n",
      "Epoch 21 Batch 1000 Loss 0.3180\n",
      "Epoch 21 Batch 1200 Loss 0.4446\n",
      "Epoch 21 Batch 1400 Loss 0.3935\n",
      "Epoch 21 Batch 1600 Loss 0.3251\n",
      "Epoch 21 Batch 1800 Loss 0.3712\n",
      "Epoch 21 Batch 2000 Loss 0.4871\n",
      "Epoch 21 Batch 2200 Loss 0.2884\n",
      "Epoch 21 Batch 2400 Loss 0.2480\n",
      "Epoch 21 Loss 0.4022\n",
      "Time taken for 1 epoch 1635.7813885211945 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an error for a request to the server \n",
      "bleu4:  4.082482905890309e-06\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.3939\n",
      "Epoch 22 Batch 200 Loss 0.3768\n",
      "Epoch 22 Batch 400 Loss 0.3423\n",
      "Epoch 22 Batch 600 Loss 0.3916\n",
      "Epoch 22 Batch 800 Loss 0.3811\n",
      "Epoch 22 Batch 1000 Loss 0.4119\n",
      "Epoch 22 Batch 1200 Loss 0.2186\n",
      "Epoch 22 Batch 1400 Loss 0.3544\n",
      "Epoch 22 Batch 1600 Loss 0.3513\n",
      "Epoch 22 Batch 1800 Loss 0.2544\n",
      "Epoch 22 Batch 2000 Loss 0.3489\n",
      "Epoch 22 Batch 2200 Loss 0.4242\n",
      "Epoch 22 Batch 2400 Loss 0.2691\n",
      "Epoch 22 Loss 0.3838\n",
      "Time taken for 1 epoch 1632.4738411903381 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an event to the server to confirm that will not be notified each test is initialized to the server \n",
      "bleu4:  2.6934666345775126e-06\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.2568\n",
      "Epoch 23 Batch 200 Loss 0.5118\n",
      "Epoch 23 Batch 400 Loss 0.3931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 600 Loss 0.2600\n",
      "Epoch 23 Batch 800 Loss 0.1658\n",
      "Epoch 23 Batch 1000 Loss 0.3878\n",
      "Epoch 23 Batch 1200 Loss 0.3275\n",
      "Epoch 23 Batch 1400 Loss 0.3056\n",
      "Epoch 23 Batch 1600 Loss 0.3554\n",
      "Epoch 23 Batch 1800 Loss 0.3643\n",
      "Epoch 23 Batch 2000 Loss 0.2926\n",
      "Epoch 23 Batch 2200 Loss 0.3027\n",
      "Epoch 23 Batch 2400 Loss 0.3711\n",
      "Epoch 23 Loss 0.3723\n",
      "Time taken for 1 epoch 1634.5911796092987 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the client \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.2134\n",
      "Epoch 24 Batch 200 Loss 0.2854\n",
      "Epoch 24 Batch 400 Loss 0.1876\n",
      "Epoch 24 Batch 600 Loss 0.2935\n",
      "Epoch 24 Batch 800 Loss 0.3847\n",
      "Epoch 24 Batch 1000 Loss 0.3294\n",
      "Epoch 24 Batch 1200 Loss 0.2942\n",
      "Epoch 24 Batch 1400 Loss 0.2710\n",
      "Epoch 24 Batch 1600 Loss 0.3990\n",
      "Epoch 24 Batch 1800 Loss 0.3013\n",
      "Epoch 24 Batch 2000 Loss 0.2831\n",
      "Epoch 24 Batch 2200 Loss 0.3091\n",
      "Epoch 24 Batch 2400 Loss 0.4081\n",
      "Epoch 24 Loss 0.3539\n",
      "Time taken for 1 epoch 1633.2220213413239 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an event to the server \n",
      "bleu4:  3.6409302405825886e-06\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.3424\n",
      "Epoch 25 Batch 200 Loss 0.2840\n",
      "Epoch 25 Batch 400 Loss 0.2286\n",
      "Epoch 25 Batch 600 Loss 0.3386\n",
      "Epoch 25 Batch 800 Loss 0.3558\n",
      "Epoch 25 Batch 1000 Loss 0.1785\n",
      "Epoch 25 Batch 1200 Loss 0.3761\n",
      "Epoch 25 Batch 1400 Loss 0.3809\n",
      "Epoch 25 Batch 1600 Loss 0.3073\n",
      "Epoch 25 Batch 1800 Loss 0.2992\n",
      "Epoch 25 Batch 2000 Loss 0.3178\n",
      "Epoch 25 Batch 2200 Loss 0.3018\n",
      "Epoch 25 Batch 2400 Loss 0.4106\n",
      "Epoch 25 Loss 0.3374\n",
      "Time taken for 1 epoch 1633.146118402481 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.2783\n",
      "Epoch 26 Batch 200 Loss 0.3739\n",
      "Epoch 26 Batch 400 Loss 0.3273\n",
      "Epoch 26 Batch 600 Loss 0.2827\n",
      "Epoch 26 Batch 800 Loss 0.4809\n",
      "Epoch 26 Batch 1000 Loss 0.3841\n",
      "Epoch 26 Batch 1200 Loss 0.3831\n",
      "Epoch 26 Batch 1400 Loss 0.3199\n",
      "Epoch 26 Batch 1600 Loss 0.2821\n",
      "Epoch 26 Batch 1800 Loss 0.3746\n",
      "Epoch 26 Batch 2000 Loss 0.3568\n",
      "Epoch 26 Batch 2200 Loss 0.2603\n",
      "Epoch 26 Batch 2400 Loss 0.3525\n",
      "Epoch 26 Loss 0.3247\n",
      "Time taken for 1 epoch 1633.1046090126038 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an event \n",
      "bleu4:  1.4351442320726098e-06\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.3251\n",
      "Epoch 27 Batch 200 Loss 0.2916\n",
      "Epoch 27 Batch 400 Loss 0.2967\n",
      "Epoch 27 Batch 600 Loss 0.2910\n",
      "Epoch 27 Batch 800 Loss 0.2386\n",
      "Epoch 27 Batch 1000 Loss 0.3165\n",
      "Epoch 27 Batch 1200 Loss 0.3832\n",
      "Epoch 27 Batch 1400 Loss 0.2035\n",
      "Epoch 27 Batch 1600 Loss 0.2923\n",
      "Epoch 27 Batch 1800 Loss 0.2925\n",
      "Epoch 27 Batch 2000 Loss 0.2495\n",
      "Epoch 27 Batch 2200 Loss 0.4422\n",
      "Epoch 27 Batch 2400 Loss 0.3146\n",
      "Epoch 27 Loss 0.3115\n",
      "Time taken for 1 epoch 1628.4644434452057 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a message to the server \n",
      "bleu4:  1.4477626228439056e-08\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.3681\n",
      "Epoch 28 Batch 200 Loss 0.3591\n",
      "Epoch 28 Batch 400 Loss 0.4051\n",
      "Epoch 28 Batch 600 Loss 0.2458\n",
      "Epoch 28 Batch 800 Loss 0.4063\n",
      "Epoch 28 Batch 1000 Loss 0.3748\n",
      "Epoch 28 Batch 1200 Loss 0.2059\n",
      "Epoch 28 Batch 1400 Loss 0.2775\n",
      "Epoch 28 Batch 1600 Loss 0.2810\n",
      "Epoch 28 Batch 1800 Loss 0.2177\n",
      "Epoch 28 Batch 2000 Loss 0.3695\n",
      "Epoch 28 Batch 2200 Loss 0.3631\n",
      "Epoch 28 Batch 2400 Loss 0.2851\n",
      "Epoch 28 Loss 0.2988\n",
      "Time taken for 1 epoch 1630.9802961349487 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an error listener that we can easily run method directly from the server to avoid memory \n",
      "bleu4:  2.9282980154993997e-06\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.3095\n",
      "Epoch 29 Batch 200 Loss 0.1835\n",
      "Epoch 29 Batch 400 Loss 0.2744\n",
      "Epoch 29 Batch 600 Loss 0.3422\n",
      "Epoch 29 Batch 800 Loss 0.3323\n",
      "Epoch 29 Batch 1000 Loss 0.2834\n",
      "Epoch 29 Batch 1200 Loss 0.2632\n",
      "Epoch 29 Batch 1400 Loss 0.2394\n",
      "Epoch 29 Batch 1600 Loss 0.1708\n",
      "Epoch 29 Batch 1800 Loss 0.2647\n",
      "Epoch 29 Batch 2000 Loss 0.2685\n",
      "Epoch 29 Batch 2200 Loss 0.2356\n",
      "Epoch 29 Batch 2400 Loss 0.3216\n",
      "Epoch 29 Loss 0.2870\n",
      "Time taken for 1 epoch 1637.322618484497 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an error listener to all registered listeners \n",
      "bleu4:  4.347208720670622e-06\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.2006\n",
      "Epoch 30 Batch 200 Loss 0.3318\n",
      "Epoch 30 Batch 400 Loss 0.2132\n",
      "Epoch 30 Batch 600 Loss 0.2067\n",
      "Epoch 30 Batch 800 Loss 0.1997\n",
      "Epoch 30 Batch 1000 Loss 0.2582\n",
      "Epoch 30 Batch 1200 Loss 0.2858\n",
      "Epoch 30 Batch 1400 Loss 0.3420\n",
      "Epoch 30 Batch 1600 Loss 0.3186\n",
      "Epoch 30 Batch 1800 Loss 0.3086\n",
      "Epoch 30 Batch 2000 Loss 0.3368\n",
      "Epoch 30 Batch 2200 Loss 0.2138\n",
      "Epoch 30 Batch 2400 Loss 0.2889\n",
      "Epoch 30 Loss 0.2765\n",
      "Time taken for 1 epoch 1634.107464313507 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends a key producer to queue to shared configuration \n",
      "bleu4:  1.8257418587562708e-08\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.1376\n",
      "Epoch 31 Batch 200 Loss 0.1816\n",
      "Epoch 31 Batch 400 Loss 0.1915\n",
      "Epoch 31 Batch 600 Loss 0.2696\n",
      "Epoch 31 Batch 800 Loss 0.2192\n",
      "Epoch 31 Batch 1000 Loss 0.2095\n",
      "Epoch 31 Batch 1200 Loss 0.2720\n",
      "Epoch 31 Batch 1400 Loss 0.3244\n",
      "Epoch 31 Batch 1600 Loss 0.3117\n",
      "Epoch 31 Batch 1800 Loss 0.1011\n",
      "Epoch 31 Batch 2000 Loss 0.2641\n",
      "Epoch 31 Batch 2200 Loss 0.2443\n",
      "Epoch 31 Batch 2400 Loss 0.3101\n",
      "Epoch 31 Loss 0.2662\n",
      "Time taken for 1 epoch 1630.2870738506317 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an object that will not unset the equipment is not supported or unlock it to be sent due to the server will be used for testing \n",
      "bleu4:  2.3103257643366804e-06\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.2301\n",
      "Epoch 32 Batch 200 Loss 0.3849\n",
      "Epoch 32 Batch 400 Loss 0.1871\n",
      "Epoch 32 Batch 600 Loss 0.2458\n",
      "Epoch 32 Batch 800 Loss 0.2368\n",
      "Epoch 32 Batch 1000 Loss 0.3271\n",
      "Epoch 32 Batch 1200 Loss 0.3248\n",
      "Epoch 32 Batch 1400 Loss 0.2069\n",
      "Epoch 32 Batch 1600 Loss 0.2829\n",
      "Epoch 32 Batch 1800 Loss 0.1985\n",
      "Epoch 32 Batch 2000 Loss 0.2889\n",
      "Epoch 32 Batch 2200 Loss 0.2020\n",
      "Epoch 32 Batch 2400 Loss 0.3574\n",
      "Epoch 32 Loss 0.2550\n",
      "Time taken for 1 epoch 1632.322205066681 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update '' packet \n",
      "bleu4:  0.402935166805056\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.1752\n",
      "Epoch 33 Batch 200 Loss 0.1756\n",
      "Epoch 33 Batch 400 Loss 0.2510\n",
      "Epoch 33 Batch 600 Loss 0.2445\n",
      "Epoch 33 Batch 800 Loss 0.3287\n",
      "Epoch 33 Batch 1000 Loss 0.2403\n",
      "Epoch 33 Batch 1200 Loss 0.2570\n",
      "Epoch 33 Batch 1400 Loss 0.2882\n",
      "Epoch 33 Batch 1600 Loss 0.1939\n",
      "Epoch 33 Batch 1800 Loss 0.3625\n",
      "Epoch 33 Batch 2000 Loss 0.2376\n",
      "Epoch 33 Batch 2200 Loss 0.3216\n",
      "Epoch 33 Batch 2400 Loss 0.2905\n",
      "Epoch 33 Loss 0.2464\n",
      "Time taken for 1 epoch 1636.5572531223297 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update entity '' packet \n",
      "bleu4:  0.39442436492702493\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.2409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 400 Loss 0.2331\n",
      "Epoch 34 Batch 600 Loss 0.1860\n",
      "Epoch 34 Batch 800 Loss 0.3457\n",
      "Epoch 34 Batch 1000 Loss 0.1576\n",
      "Epoch 34 Batch 1200 Loss 0.1683\n",
      "Epoch 34 Batch 1400 Loss 0.2368\n",
      "Epoch 34 Batch 1600 Loss 0.2688\n",
      "Epoch 34 Batch 1800 Loss 0.3051\n",
      "Epoch 34 Batch 2000 Loss 0.3564\n",
      "Epoch 34 Batch 2200 Loss 0.2251\n",
      "Epoch 34 Batch 2400 Loss 0.3176\n",
      "Epoch 34 Loss 0.2369\n",
      "Time taken for 1 epoch 1635.4436950683594 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update can receive all headers to the server with negative ) \n",
      "bleu4:  0.17778351189237493\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.1744\n",
      "Epoch 35 Batch 200 Loss 0.1879\n",
      "Epoch 35 Batch 400 Loss 0.1680\n",
      "Epoch 35 Batch 600 Loss 0.2875\n",
      "Epoch 35 Batch 800 Loss 0.2459\n",
      "Epoch 35 Batch 1000 Loss 0.2351\n",
      "Epoch 35 Batch 1200 Loss 0.2451\n",
      "Epoch 35 Batch 1400 Loss 0.1633\n",
      "Epoch 35 Batch 1600 Loss 0.1815\n",
      "Epoch 35 Batch 1800 Loss 0.2438\n",
      "Epoch 35 Batch 2000 Loss 0.2262\n",
      "Epoch 35 Batch 2200 Loss 0.3261\n",
      "Epoch 35 Batch 2400 Loss 0.1483\n",
      "Epoch 35 Loss 0.2280\n",
      "Time taken for 1 epoch 1631.9907205104828 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update entity '' packet \n",
      "bleu4:  0.39442436492702493\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.2327\n",
      "Epoch 36 Batch 200 Loss 0.2427\n",
      "Epoch 36 Batch 400 Loss 0.2202\n",
      "Epoch 36 Batch 600 Loss 0.1533\n",
      "Epoch 36 Batch 800 Loss 0.2656\n",
      "Epoch 36 Batch 1000 Loss 0.1936\n",
      "Epoch 36 Batch 1200 Loss 0.1686\n",
      "Epoch 36 Batch 1400 Loss 0.2118\n",
      "Epoch 36 Batch 1600 Loss 0.1381\n",
      "Epoch 36 Batch 1800 Loss 0.1172\n",
      "Epoch 36 Batch 2000 Loss 0.2024\n",
      "Epoch 36 Batch 2200 Loss 0.2634\n",
      "Epoch 36 Batch 2400 Loss 0.2369\n",
      "Epoch 36 Loss 0.2204\n",
      "Time taken for 1 epoch 1633.2251632213593 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update '' packet \n",
      "bleu4:  0.402935166805056\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.1498\n",
      "Epoch 37 Batch 200 Loss 0.1556\n",
      "Epoch 37 Batch 400 Loss 0.2453\n",
      "Epoch 37 Batch 600 Loss 0.2055\n",
      "Epoch 37 Batch 800 Loss 0.2089\n",
      "Epoch 37 Batch 1000 Loss 0.2535\n",
      "Epoch 37 Batch 1200 Loss 0.1219\n",
      "Epoch 37 Batch 1400 Loss 0.1543\n",
      "Epoch 37 Batch 1600 Loss 0.3356\n",
      "Epoch 37 Batch 1800 Loss 0.2514\n",
      "Epoch 37 Batch 2000 Loss 0.1967\n",
      "Epoch 37 Batch 2200 Loss 0.2773\n",
      "Epoch 37 Batch 2400 Loss 0.2418\n",
      "Epoch 37 Loss 0.2118\n",
      "Time taken for 1 epoch 1636.920639038086 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an object to use directly from your output handler \n",
      "bleu4:  3.860973952291859e-06\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.1089\n",
      "Epoch 38 Batch 200 Loss 0.1573\n",
      "Epoch 38 Batch 400 Loss 0.2172\n",
      "Epoch 38 Batch 600 Loss 0.1549\n",
      "Epoch 38 Batch 800 Loss 0.2501\n",
      "Epoch 38 Batch 1000 Loss 0.2358\n",
      "Epoch 38 Batch 1200 Loss 0.2910\n",
      "Epoch 38 Batch 1400 Loss 0.1772\n",
      "Epoch 38 Batch 1600 Loss 0.1826\n",
      "Epoch 38 Batch 1800 Loss 0.1820\n",
      "Epoch 38 Batch 2000 Loss 0.2772\n",
      "Epoch 38 Batch 2200 Loss 0.2986\n",
      "Epoch 38 Batch 2400 Loss 0.2361\n",
      "Epoch 38 Loss 0.2049\n",
      "Time taken for 1 epoch 1629.6190767288208 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update can not be used to add it to avoid memory \n",
      "bleu4:  0.17778351189237493\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.1120\n",
      "Epoch 39 Batch 200 Loss 0.2364\n",
      "Epoch 39 Batch 400 Loss 0.1868\n",
      "Epoch 39 Batch 600 Loss 0.2655\n",
      "Epoch 39 Batch 800 Loss 0.2601\n",
      "Epoch 39 Batch 1000 Loss 0.2496\n",
      "Epoch 39 Batch 1200 Loss 0.1407\n",
      "Epoch 39 Batch 1400 Loss 0.2620\n",
      "Epoch 39 Batch 1600 Loss 0.1529\n",
      "Epoch 39 Batch 1800 Loss 0.1515\n",
      "Epoch 39 Batch 2000 Loss 0.1532\n",
      "Epoch 39 Batch 2200 Loss 0.1983\n",
      "Epoch 39 Batch 2400 Loss 0.1818\n",
      "Epoch 39 Loss 0.1979\n",
      "Time taken for 1 epoch 1630.1983828544617 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update custom initiative '' packet \n",
      "bleu4:  0.7311104458083122\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0515\n",
      "Epoch 40 Batch 200 Loss 0.1230\n",
      "Epoch 40 Batch 400 Loss 0.1846\n",
      "Epoch 40 Batch 600 Loss 0.1344\n",
      "Epoch 40 Batch 800 Loss 0.0953\n",
      "Epoch 40 Batch 1000 Loss 0.1139\n",
      "Epoch 40 Batch 1200 Loss 0.1281\n",
      "Epoch 40 Batch 1400 Loss 0.2426\n",
      "Epoch 40 Batch 1600 Loss 0.2478\n",
      "Epoch 40 Batch 1800 Loss 0.2295\n",
      "Epoch 40 Batch 2000 Loss 0.2450\n",
      "Epoch 40 Batch 2200 Loss 0.1698\n",
      "Epoch 40 Batch 2400 Loss 0.2272\n",
      "Epoch 40 Loss 0.1911\n",
      "Time taken for 1 epoch 1632.4456102848053 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an error listener to queue \n",
      "bleu4:  3.6409302405825886e-06\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.1275\n",
      "Epoch 41 Batch 200 Loss 0.1684\n",
      "Epoch 41 Batch 400 Loss 0.1533\n",
      "Epoch 41 Batch 600 Loss 0.1995\n",
      "Epoch 41 Batch 800 Loss 0.1815\n",
      "Epoch 41 Batch 1000 Loss 0.2102\n",
      "Epoch 41 Batch 1200 Loss 0.1291\n",
      "Epoch 41 Batch 1400 Loss 0.1287\n",
      "Epoch 41 Batch 1600 Loss 0.1614\n",
      "Epoch 41 Batch 1800 Loss 0.1174\n",
      "Epoch 41 Batch 2000 Loss 0.1649\n",
      "Epoch 41 Batch 2200 Loss 0.1436\n",
      "Epoch 41 Batch 2400 Loss 0.1330\n",
      "Epoch 41 Loss 0.1851\n",
      "Time taken for 1 epoch 1632.9031021595001 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update entity '' packet \n",
      "bleu4:  0.39442436492702493\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.1584\n",
      "Epoch 42 Batch 200 Loss 0.2223\n",
      "Epoch 42 Batch 400 Loss 0.1338\n",
      "Epoch 42 Batch 600 Loss 0.1912\n",
      "Epoch 42 Batch 800 Loss 0.1227\n",
      "Epoch 42 Batch 1000 Loss 0.1464\n",
      "Epoch 42 Batch 1200 Loss 0.1244\n",
      "Epoch 42 Batch 1400 Loss 0.2336\n",
      "Epoch 42 Batch 1600 Loss 0.1894\n",
      "Epoch 42 Batch 1800 Loss 0.1973\n",
      "Epoch 42 Batch 2000 Loss 0.1988\n",
      "Epoch 42 Batch 2200 Loss 0.1436\n",
      "Epoch 42 Batch 2400 Loss 0.1650\n",
      "Epoch 42 Loss 0.1787\n",
      "Time taken for 1 epoch 1634.3315889835358 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update custom initiative '' packet \n",
      "bleu4:  0.7311104458083122\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.1302\n",
      "Epoch 43 Batch 200 Loss 0.1477\n",
      "Epoch 43 Batch 400 Loss 0.1603\n",
      "Epoch 43 Batch 600 Loss 0.0706\n",
      "Epoch 43 Batch 800 Loss 0.2495\n",
      "Epoch 43 Batch 1000 Loss 0.1075\n",
      "Epoch 43 Batch 1200 Loss 0.1532\n",
      "Epoch 43 Batch 1400 Loss 0.1920\n",
      "Epoch 43 Batch 1600 Loss 0.1571\n",
      "Epoch 43 Batch 1800 Loss 0.1204\n",
      "Epoch 43 Batch 2000 Loss 0.1901\n",
      "Epoch 43 Batch 2200 Loss 0.1977\n",
      "Epoch 43 Batch 2400 Loss 0.1729\n",
      "Epoch 43 Loss 0.1740\n",
      "Time taken for 1 epoch 1632.2132229804993 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update custom initiative '' packet \n",
      "bleu4:  0.7311104458083122\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1025\n",
      "Epoch 44 Batch 200 Loss 0.1480\n",
      "Epoch 44 Batch 400 Loss 0.0997\n",
      "Epoch 44 Batch 600 Loss 0.2595\n",
      "Epoch 44 Batch 800 Loss 0.1312\n",
      "Epoch 44 Batch 1000 Loss 0.1224\n",
      "Epoch 44 Batch 1200 Loss 0.1573\n",
      "Epoch 44 Batch 1400 Loss 0.1706\n",
      "Epoch 44 Batch 1600 Loss 0.1132\n",
      "Epoch 44 Batch 1800 Loss 0.1154\n",
      "Epoch 44 Batch 2000 Loss 0.1243\n",
      "Epoch 44 Batch 2200 Loss 0.1644\n",
      "Epoch 44 Batch 2400 Loss 0.1646\n",
      "Epoch 44 Loss 0.1672\n",
      "Time taken for 1 epoch 1632.6765224933624 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update custom initiative '' packet \n",
      "bleu4:  0.7311104458083122\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.2132\n",
      "Epoch 45 Batch 200 Loss 0.2327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 400 Loss 0.2325\n",
      "Epoch 45 Batch 600 Loss 0.1547\n",
      "Epoch 45 Batch 800 Loss 0.1034\n",
      "Epoch 45 Batch 1000 Loss 0.2380\n",
      "Epoch 45 Batch 1200 Loss 0.0953\n",
      "Epoch 45 Batch 1400 Loss 0.1347\n",
      "Epoch 45 Batch 1600 Loss 0.1447\n",
      "Epoch 45 Batch 1800 Loss 0.2236\n",
      "Epoch 45 Batch 2000 Loss 0.1816\n",
      "Epoch 45 Batch 2200 Loss 0.0854\n",
      "Epoch 45 Batch 2400 Loss 0.1982\n",
      "Epoch 45 Loss 0.1630\n",
      "Time taken for 1 epoch 1632.7496206760406 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update custom initiative '' packet \n",
      "bleu4:  0.7311104458083122\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0958\n",
      "Epoch 46 Batch 200 Loss 0.1337\n",
      "Epoch 46 Batch 400 Loss 0.1164\n",
      "Epoch 46 Batch 600 Loss 0.1604\n",
      "Epoch 46 Batch 800 Loss 0.2205\n",
      "Epoch 46 Batch 1000 Loss 0.1579\n",
      "Epoch 46 Batch 1200 Loss 0.1141\n",
      "Epoch 46 Batch 1400 Loss 0.0837\n",
      "Epoch 46 Batch 1600 Loss 0.0954\n",
      "Epoch 46 Batch 1800 Loss 0.0776\n",
      "Epoch 46 Batch 2000 Loss 0.1666\n",
      "Epoch 46 Batch 2200 Loss 0.1784\n",
      "Epoch 46 Batch 2400 Loss 0.1386\n",
      "Epoch 46 Loss 0.1565\n",
      "Time taken for 1 epoch 1635.2537133693695 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update can pass a message and closes all client history \n",
      "bleu4:  0.19338531392590738\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.1278\n",
      "Epoch 47 Batch 200 Loss 0.1722\n",
      "Epoch 47 Batch 400 Loss 0.1077\n",
      "Epoch 47 Batch 600 Loss 0.2314\n",
      "Epoch 47 Batch 800 Loss 0.1243\n",
      "Epoch 47 Batch 1000 Loss 0.1550\n",
      "Epoch 47 Batch 1200 Loss 0.1216\n",
      "Epoch 47 Batch 1400 Loss 0.1481\n",
      "Epoch 47 Batch 1600 Loss 0.1019\n",
      "Epoch 47 Batch 1800 Loss 0.1829\n",
      "Epoch 47 Batch 2000 Loss 0.1504\n",
      "Epoch 47 Batch 2200 Loss 0.0836\n",
      "Epoch 47 Batch 2400 Loss 0.1842\n",
      "Epoch 47 Loss 0.1529\n",
      "Time taken for 1 epoch 1632.3598001003265 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update can pass as they become specific message \n",
      "bleu4:  0.23462350331177745\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0949\n",
      "Epoch 48 Batch 200 Loss 0.1169\n",
      "Epoch 48 Batch 400 Loss 0.1275\n",
      "Epoch 48 Batch 600 Loss 0.0998\n",
      "Epoch 48 Batch 800 Loss 0.1656\n",
      "Epoch 48 Batch 1000 Loss 0.1506\n",
      "Epoch 48 Batch 1200 Loss 0.0810\n",
      "Epoch 48 Batch 1400 Loss 0.1880\n",
      "Epoch 48 Batch 1600 Loss 0.1945\n",
      "Epoch 48 Batch 1800 Loss 0.1989\n",
      "Epoch 48 Batch 2000 Loss 0.1403\n",
      "Epoch 48 Batch 2200 Loss 0.1452\n",
      "Epoch 48 Batch 2400 Loss 0.1780\n",
      "Epoch 48 Loss 0.1479\n",
      "Time taken for 1 epoch 1634.10897731781 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update '' packet \n",
      "bleu4:  0.402935166805056\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0814\n",
      "Epoch 49 Batch 200 Loss 0.0664\n",
      "Epoch 49 Batch 400 Loss 0.2003\n",
      "Epoch 49 Batch 600 Loss 0.1032\n",
      "Epoch 49 Batch 800 Loss 0.1475\n",
      "Epoch 49 Batch 1000 Loss 0.1292\n",
      "Epoch 49 Batch 1200 Loss 0.1740\n",
      "Epoch 49 Batch 1400 Loss 0.1366\n",
      "Epoch 49 Batch 1600 Loss 0.2670\n",
      "Epoch 49 Batch 1800 Loss 0.1530\n",
      "Epoch 49 Batch 2000 Loss 0.1906\n",
      "Epoch 49 Batch 2200 Loss 0.1625\n",
      "Epoch 49 Batch 2400 Loss 0.1379\n",
      "Epoch 49 Loss 0.1539\n",
      "Time taken for 1 epoch 1633.7528030872345 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update custom initiative '' packet \n",
      "bleu4:  0.7311104458083122\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.1330\n",
      "Epoch 50 Batch 200 Loss 0.1079\n",
      "Epoch 50 Batch 400 Loss 0.2118\n",
      "Epoch 50 Batch 600 Loss 0.0983\n",
      "Epoch 50 Batch 800 Loss 0.1272\n",
      "Epoch 50 Batch 1000 Loss 0.1674\n",
      "Epoch 50 Batch 1200 Loss 0.2042\n",
      "Epoch 50 Batch 1400 Loss 0.1564\n",
      "Epoch 50 Batch 1600 Loss 0.1375\n",
      "Epoch 50 Batch 1800 Loss 0.1461\n",
      "Epoch 50 Batch 2000 Loss 0.1078\n",
      "Epoch 50 Batch 2200 Loss 0.2067\n",
      "Epoch 50 Batch 2400 Loss 0.1838\n",
      "Epoch 50 Loss 0.1441\n",
      "Time taken for 1 epoch 1634.7101790904999 sec\n",
      "\n",
      "public void sendCustomInit(IPlayer player){\n",
      "  send(new Packet(Packet.COMMAND_CUSTOM_INITIATIVE,player));\n",
      "}\n",
      "Original comment:  Sends an \"update custom initiative\" packet\n",
      "Predictedd translation:  Sends an `` update entity '' packet \n",
      "bleu4:  0.39442436492702493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden_h, hidden_c = encoder.initialize_hidden_state()\n",
    "    \n",
    "    hidden = [hidden_h, hidden_c]\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "\n",
    "    code_train_batch = getBatch(code_train, BATCH_SIZE)\n",
    "    \n",
    "    comment_train_batch = getBatch(comment_train, BATCH_SIZE)\n",
    "    \n",
    "    dataset = [(code_train_batch[i], comment_train_batch[i]) for i in range(0, len(code_train_batch))]\n",
    "    \n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "            \n",
    "            dec_input = tf.expand_dims([comment_voc.index('<START>')] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(0, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 200 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    lossArray = np.append(lossArray, (total_loss / N_BATCH) )\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    # ============ can remove below ===============\n",
    "    index = 5\n",
    "    print(test_inputs[index], end=\"\")\n",
    "    print(\"Original comment: \",test_outputs[index], end=\"\\n\")\n",
    "    predict = simple_translate(test_inputs[index], encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ)\n",
    "    print(\"Predictedd translation: \", predict, end=\"\\n\")\n",
    "    bleu4_score = bleu4(test_outputs[index], predict)\n",
    "    print(\"bleu4: \", bleu4_score, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, EPOCHS+1), lossArray, \"b\", label=\"train\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Adam Optimizer model loss\")\n",
    "plt.legend(loc = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"tmp\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void mouseClicked(MouseEvent e){\n",
      "  if (SwingUtilities.isRightMouseButton(e))   m_adaptee.popupMenu.show((Component)e.getSource(),e.getX(),e.getY());\n",
      "}\n",
      "\n",
      "\n",
      "Original comment:  Mouse Listener\n",
      "\n",
      "Predictedd translation:  Mouse Listener \n",
      "\n",
      "bleu4:  1.0000000000625001e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 18\n",
    "print(test_inputs[index], end=\"\\n\\n\")\n",
    "print(\"Original comment: \",test_outputs[index], end=\"\\n\\n\")\n",
    "predict = simple_translate(test_inputs[index], encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ)\n",
    "print(\"Predictedd translation: \", predict, end=\"\\n\\n\")\n",
    "bleu4_score = bleu4(test_outputs[index], predict)\n",
    "print(\"bleu4: \", bleu4_score, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  8,  9, 10, 11, 12, 13,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_voc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus = []\n",
    "\n",
    "for index in range(len(test_inputs)):\n",
    "    predict = translate(test_inputs[index], encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ)\n",
    "    bleus += [bleu4(test_outputs[index], predict)]\n",
    "np.mean(bleus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
