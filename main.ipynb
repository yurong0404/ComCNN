{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yurong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import javalang\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "MODE = \"SBT\"  #normal or simple or SBT\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function：\n",
    "    Input the code token list, comment string，and return whether it's invalid\n",
    "The rule of valid method:\n",
    "    1. code token size <= 100\n",
    "    2. One-sentence-comment（I hope model generate only one sentence.\n",
    "       If training data consist multi-sentence comment, the effect will be bad and the first sentence only can not\n",
    "       properly describe the functionality of the method）\n",
    "\n",
    "PS: I regard \"tester\", \"setter\", \"getter\" and \"constructor\" as valid method\n",
    "'''\n",
    "def is_invalid_method(token_len, nl):   \n",
    "    if token_len > 100:\n",
    "        return True\n",
    "    if len(nl.split('.')) != 1 or len(nltk.word_tokenize(nl)) > 30:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "    \n",
    "'''\n",
    "Function: \n",
    "    Input the root of AST and the deep of the tree, \n",
    "    it will filter the null value and return the list of SBT (structural-based travesal) and print the tree structure\n",
    "'''\n",
    "def parse_tree(root, deep):\n",
    "    seq = []\n",
    "    seq.extend(['(', str(root).split('(')[0]])\n",
    "    #print('\\t'*(deep)+str(root).split('(')[0])    # show node name\n",
    "    if not hasattr(root, 'attrs'):  # error-handling\n",
    "        return []\n",
    "    for attr in root.attrs:\n",
    "        if eval('root.%s' % attr) in [None, [], \"\", set(), False]:    # filter the null attr\n",
    "            continue\n",
    "        elif isinstance(eval('root.%s' % attr), list):\n",
    "            x = eval('root.%s' % attr)\n",
    "            if not all(elem in x for elem in [None, [], \"\", set(), False]):    # if not all elements in list are null\n",
    "                seq.extend(['(',attr])\n",
    "                #print('\\t'*(deep+1)+attr)\n",
    "                #deep += 1\n",
    "                for i in eval('root.%s' % attr):    # recursive the list\n",
    "                    if i is None or isinstance(i, str):    # perhaps it has None value in the list\n",
    "                        continue\n",
    "                    #deep += 1\n",
    "                    seq.extend(parse_tree(i, deep))\n",
    "                    \n",
    "                    #deep -= 1\n",
    "                #deep -= 1\n",
    "                seq.extend([')',attr])\n",
    "        elif 'tree' in str(type(eval('root.%s' % attr))):    #if the attr is one kind of Node, recursive the Node\n",
    "            seq.extend(['(',attr])\n",
    "            #print('\\t'*(deep+1)+attr)\n",
    "            #deep += 2\n",
    "            seq.extend(parse_tree(eval('root.%s' % attr), deep))\n",
    "            #deep -= 2\n",
    "            seq.extend([')',attr])\n",
    "        else:\n",
    "            seq.extend(['(','<'+str(attr)+'>_'+str(eval('root.%s' % attr)),')','<'+str(attr)+'>_'+str(eval('root.%s' % attr))])\n",
    "            #exec(\"print('\\t'*(deep+1)+attr+': '+str(root.%s))\" % attr)    #it must be normal attribute\n",
    "    seq.extend([')', str(root).split('(')[0]])\n",
    "    return seq\n",
    "\n",
    "\n",
    "'''\n",
    "Usage:\n",
    "    1. \"camelCase\" -> [\"camel\", \"Case\"]\n",
    "    2. \"under_score\" -> [\"under\", \"_\", \"score\"]\n",
    "    3. \"normal\" -> [\"normal\"]\n",
    "'''\n",
    "def split_identifier(id_token):\n",
    "    if  \"_\" in id_token:\n",
    "        return id_token.split(\"_\")\n",
    "    elif id_token != id_token.lower() and id_token != id_token.upper():\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', id_token)\n",
    "        return [m.group(0) for m in matches]\n",
    "    else:\n",
    "        return [id_token]\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "Usage:\n",
    "    1. input the list of train, test, valid dataset\n",
    "    2. filter the dataset, split it to train, test, valid set and save as the smaller dataset.\n",
    "    3. return the amount of the data from smaller datasets.\n",
    "Example:\n",
    "    filter_dataset(['./data/train.json', './data/test.json', './data/valid.json'], './data')\n",
    "Note:\n",
    "    The filter method is different from the method in DeepCom, because I have no idea how DeepCom did.\n",
    "    It doesn't make sense that DeepCom could filter so many data via the method mentioned in its paper.\n",
    "'''\n",
    "def filter_dataset(path_list, save_path):\n",
    "    \n",
    "    inputs = []\n",
    "    for path in path_list:\n",
    "        input_file = open(path)\n",
    "        inputs.extend(input_file.readlines())\n",
    "        input_file.close()\n",
    "    outputs = []\n",
    "    output_train_file = open(save_path+'/filter_train.json', \"w\")\n",
    "    output_test_file = open(save_path+'/filter_test.json', \"w\")\n",
    "    output_valid_file = open(save_path+'/filter_valid.json', \"w\")\n",
    "    \n",
    "    print('Original total: '+str(len(inputs)))\n",
    "    for pair in inputs:\n",
    "        pair = json.loads(pair)\n",
    "        tokens_parse = javalang.tokenizer.tokenize(pair['code'])\n",
    "        if is_invalid_method(len(list(tokens_parse)), pair['nl']):\n",
    "            continue\n",
    "        outputs.append(json.dumps(pair))\n",
    "\n",
    "    random.shuffle(outputs)\n",
    "    print('Final total: '+str(len(outputs)))\n",
    "    print('Data shuffle complete')\n",
    "    train_index = int(len(outputs)*0.8)\n",
    "    test_index = int(len(outputs)*0.9)\n",
    "    train_output = outputs[:train_index]\n",
    "    test_output = outputs[train_index+1:test_index]\n",
    "    valid_output = outputs[test_index+1:]\n",
    "    \n",
    "    for row in train_output:\n",
    "        output_train_file.write(row+'\\n')\n",
    "    output_train_file.close()\n",
    "    print('filter train data finish writing')\n",
    "    for row in test_output:\n",
    "        output_test_file.write(row+'\\n')\n",
    "    output_test_file.close()\n",
    "    print('filter test data finish writing')\n",
    "    for row in valid_output:\n",
    "        output_valid_file.write(row+'\\n')\n",
    "    print('filter valid data finish writing')\n",
    "    output_valid_file.close()\n",
    "\n",
    "    return len(train_output), len(test_output), len(valid_output)\n",
    "\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "    path: the path of the data you want to read\n",
    "    code_voc: code vocabulary, the data type is list\n",
    "    comment_voc: comment vocabulary, the data type is list\n",
    "    mode: \"simple\" or \"normal\"\n",
    "Return values:\n",
    "    code_tokens, comment_tokens: 2-dimension list, store the code and comment into list, snippet by snippet\n",
    "    code_voc, comment_voc: the all vocabularies in the file of the path, data type is list\n",
    "Note:\n",
    "    It hasn't used SBT in DeepCom.\n",
    "TODO:\n",
    "    Change the rare words in comments into other common words via pre-trained embedding\n",
    "'''\n",
    "def readdata(path, code_voc, comment_voc, mode):\n",
    "    input_file = open(path)\n",
    "    inputs = input_file.readlines()\n",
    "\n",
    "    code_tokens = []          # code_tokens = ['<START>', '<Modifier>', 'public', '<Identifier>',....]\n",
    "    comment_tokens = []       # comment_tokens = []\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    #=============== extract comment part of the snippet ==========================\n",
    "    print(\"comment tokenizing...\")\n",
    "    for index, pair in enumerate(inputs):\n",
    "        pair = json.loads(pair)\n",
    "        tokens = nltk.word_tokenize(pair['nl'])\n",
    "        tokens.append('<END>')\n",
    "        comment_tokens.append(tokens)\n",
    "        for x in tokens:\n",
    "            if x not in comment_voc:\n",
    "                comment_voc.append(x)\n",
    "    \n",
    "    # =============== extract the code part of the snippet =========================\n",
    "    if mode==\"SBT\":\n",
    "        token_count = dict()\n",
    "\n",
    "        # count the code tokens\n",
    "        print(\"counting tokens...\")\n",
    "        for index, pair in enumerate(inputs):\n",
    "            if index%20000 == 0 and index != 0:\n",
    "                print(index)\n",
    "            pair = json.loads(pair)\n",
    "            parsed_inputs = code_tokenize(pair['code'], mode)\n",
    "            \n",
    "            inputs[index] = parsed_inputs\n",
    "            if len(parsed_inputs) == 0:  # error-handling due to dirty data when SBT mode\n",
    "                continue\n",
    "            \n",
    "            for x in parsed_inputs:\n",
    "                if x not in token_count:\n",
    "                    token_count[x] = 1\n",
    "                else:\n",
    "                    token_count[x] += 1\n",
    "\n",
    "        # select most frequency 30000 voc\n",
    "        typename = ['<modifiers>', '<member>', '<value>', '<name>', '<operator>', '<qualifier>']\n",
    "        code_voc.extend(typename)\n",
    "        for w in sorted(token_count, key=token_count.get, reverse=True)[:30000-len(code_voc)]:\n",
    "            code_voc.append(w) \n",
    "            \n",
    "        print('token processing...')\n",
    "        # <SimpleName>_extractFor -> <SimpleName>, if <SimpleName>_extractFor is outside 30000 voc\n",
    "        for index, parsed_inputs in enumerate(inputs):\n",
    "            if index%20000 == 0 and index != 0:\n",
    "                print(index)\n",
    "            if len(parsed_inputs) == 0:  \n",
    "                continue\n",
    "            for index2 in range(len(parsed_inputs)):\n",
    "                if parsed_inputs[index2] not in code_voc:\n",
    "                    tmp = parsed_inputs[index2].split('_')\n",
    "                    if len(tmp) > 1 and tmp[0] in typename:\n",
    "                        parsed_inputs[index2] = tmp[0]\n",
    "                    else:\n",
    "                        parsed_inputs[index2] = \"<UNK>\"\n",
    "            code_tokens.append(parsed_inputs)\n",
    "            \n",
    "\n",
    "    elif mode == \"simple\" or mode == \"normal\":\n",
    "        print(\"code tokenizing...\")\n",
    "        for index, pair in enumerate(inputs):\n",
    "            if index%20000 == 0 and index != 0:\n",
    "                print(index)\n",
    "            pair = json.loads(pair)\n",
    "            parsed_inputs = code_tokenize(pair['code'], mode)\n",
    "\n",
    "            for x in parsed_inputs:\n",
    "                if x not in code_voc:\n",
    "                    code_voc.append(x)\n",
    "            code_tokens.append(parsed_inputs)\n",
    "        \n",
    "\n",
    "    print('readdata:')\n",
    "    print('\\tdata amount: '+str(len(code_tokens)))\n",
    "    print('\\trun time: '+str(time.time()-start))\n",
    "\n",
    "    input_file.close()\n",
    "    return code_tokens, comment_tokens, code_voc, comment_voc\n",
    "\n",
    "\n",
    "'''\n",
    "Usage:\n",
    "    Transform the token to the index in vocabulary\n",
    "    ['<START>', '<Modifier>', 'public', ..., '<Separator>', ';', '<Separator>', '}', '<END>']\n",
    "    => [0, 7, 8, ..., 14, 29, 14, 30, 1]\n",
    "Parameter data type: \n",
    "    2-dimension list\n",
    "Return data type:\n",
    "    2-dimension list\n",
    "'''\n",
    "def token2index(lst, voc):\n",
    "    for index, seq in enumerate(lst):\n",
    "        seq_index = []\n",
    "        for token in seq:\n",
    "            seq_index.append(voc.index(token))\n",
    "        lst[index] = seq_index\n",
    "    return lst\n",
    "\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "    lst: the list of sequences to be padded\n",
    "    pad_data: the value you want to pad\n",
    "Return type:\n",
    "    numpy array\n",
    "'''\n",
    "def pad_sequences(lst, pad_data):\n",
    "    maxlen = max(len(x) for x in lst)\n",
    "    for index, seq in enumerate(lst):\n",
    "        lst[index].extend([pad_data] * (maxlen-len(seq)))\n",
    "    return np.array(lst)\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "    x: the list of data\n",
    "    batch_sz: batch size\n",
    "Return shape:\n",
    "    [None, batch_sz, None]\n",
    "Example:\n",
    "    a = [1,2,3,4,5,6,7,8,9,10]\n",
    "    a = getBatch(x=a, batch_sz=3)\n",
    "    a\n",
    "    ---output---\n",
    "    [[1,2,3], [4,5,6], [7,8,9]]\n",
    "'''\n",
    "def getBatch(x, batch_sz):\n",
    "    dataset = []\n",
    "    while(len(x)>=batch_sz):\n",
    "        dataset.append(x[:batch_sz])\n",
    "        x = x[batch_sz:]\n",
    "    if type(x) == np.ndarray:\n",
    "        return np.array(dataset)\n",
    "    elif type(x) == list:\n",
    "        return dataset\n",
    "    \n",
    "def ngram(words, n):\n",
    "    return list(zip(*(words[i:] for i in range(n))))\n",
    "\n",
    "\n",
    "#  bleu4 (n=4)\n",
    "def bleu(true, pred, n):\n",
    "    true = nltk.word_tokenize(true)\n",
    "    pred = nltk.word_tokenize(pred)\n",
    "    c = len(pred)\n",
    "    r = len(true)\n",
    "    bp = 1. if c > r else np.exp(1 - r / (c + 1e-10))\n",
    "    score = 0\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        true_ngram = set(ngram(true, i))\n",
    "        pred_ngram = ngram(pred, i)\n",
    "        if len(true_ngram)==0 or len(true_ngram)==0:\n",
    "            break\n",
    "        length = float(len(pred_ngram)) + 1e-10\n",
    "        count = sum([1. if t in true_ngram else 0. for t in pred_ngram])\n",
    "        score += math.log(1e-10 + (count / length))\n",
    "    score = math.exp(score / n)  #n就是公式的Wn\n",
    "    bleu = bp * score\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ, mode):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    inputs = code_tokenize(code, mode)\n",
    "    \n",
    "    if mode==\"simple\" or mode==\"normal\":\n",
    "        for index, token in enumerate(inputs):\n",
    "            if token not in code_voc:\n",
    "                inputs[index] = code_voc.index('<UNK>')\n",
    "            else:\n",
    "                inputs[index] = code_voc.index(token)\n",
    "                \n",
    "    elif mode==\"SBT\":\n",
    "        typename = ['<modifiers>', '<member>', '<value>', '<name>', '<operator>', '<qualifier>']\n",
    "        for index, token in enumerate(inputs):\n",
    "            if token not in code_voc:\n",
    "                tmp = token.split('_')\n",
    "                if len(tmp) > 1 and tmp[0] in typename:\n",
    "                    inputs[index] = code_voc.index(tmp[0])\n",
    "                else:\n",
    "                    inputs[index] = code_voc.index(\"<UNK>\")\n",
    "            else:\n",
    "                inputs[index] = code_voc.index(token)\n",
    "    \n",
    "         \n",
    "    \n",
    "    inputs += [code_voc.index('<PAD>')] * (max_length_inp - len(inputs))\n",
    "    inputs = np.array(inputs)\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden_h, hidden_c = tf.zeros((1, UNITS)), tf.zeros((1, UNITS))\n",
    "    hidden = [hidden_h, hidden_c]\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inputs, hidden)\n",
    "    dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "    \n",
    "    dec_input = tf.expand_dims([comment_voc.index('<START>')], 1)       \n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_h, dec_hidden_c, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "        dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if comment_voc[predicted_id] == '<END>':\n",
    "            return result, code, attention_plot\n",
    "        \n",
    "        result += comment_voc[predicted_id] + ' '\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, code, attention_plot\n",
    "\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    sns.set()\n",
    "    fig, ax = plt.subplots(figsize=(20,10)) \n",
    "    sns.heatmap(attention, xticklabels=sentence, yticklabels=predicted_sentence, ax=ax)\n",
    "    \n",
    "\n",
    "def translate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ, mode):\n",
    "    result, code, attention_plot = evaluate(code, encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ, mode)\n",
    "    #attention_plot = attention_plot[:len(result.split(' ')), :len(code_tokenize(code, mode))]\n",
    "    #attention_plot = distribution(attention_plot)\n",
    "    #plot_attention(attention_plot, code_tokenize(code, mode), result.split(' ')+['<END>'])\n",
    "    return result\n",
    "\n",
    "def code_tokenize(code, mode):\n",
    "    inputs = []\n",
    "    if mode ==\"simple\":\n",
    "        tokens_parse = javalang.tokenizer.tokenize(code)\n",
    "        for token in tokens_parse:    # iterate the tokens of the sentence\n",
    "            token = str(token).split(' ')\n",
    "            splitted_id = split_identifier(token[1].strip('\"'))    # split the camelCase and under_score\n",
    "            inputs.extend(splitted_id)\n",
    "    elif mode == \"normal\":\n",
    "        tokens_parse = javalang.tokenizer.tokenize(code)\n",
    "        for token in tokens_parse:    # iterate the tokens of the sentence\n",
    "            token = str(token).split(' ')\n",
    "            splitted_id = split_identifier(token[1].strip('\"'))    # split the camelCase and under_score\n",
    "            temp = ['<'+token[0]+'>']    # token[0] is token type, token[1] is token value\n",
    "            temp.extend(splitted_id)\n",
    "            inputs.extend(temp)\n",
    "            \n",
    "    elif mode == \"SBT\":\n",
    "        tree = javalang.parse.parse('class aa {'+code+'}')\n",
    "        _, node = list(tree)[2]    # 前兩個用來篩掉class aa{ }的部分\n",
    "        inputs = parse_tree(node, 0)\n",
    "        if len(inputs) == 0:   # error-handling due to dirty data\n",
    "            return []\n",
    "\n",
    "    inputs.insert(0, '<START>')\n",
    "    inputs.append('<END>')\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "\n",
    "'''\n",
    "用途：把一個二維的array做機率正規化\n",
    "例如：[[3,4,5],[1,2,3]] -> [[0.25, 0.33, 0.416], [0.167, 0.333, 0.5]]\n",
    "'''\n",
    "def distribution(arr):\n",
    "    new_arr = []\n",
    "    for i in arr:\n",
    "        tmp = []\n",
    "        total = sum(i)\n",
    "        for x in i:\n",
    "            tmp.append(x/total)\n",
    "        new_arr.append(tmp)\n",
    "    return np.array(new_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the original big dataset into simpler and better dataset\n",
    "* #### Size of training set, testing set and valid set ->  (81932, 10241, 10241)\n",
    "* #### If you already have \"filter_train.json\", \"filter_test.json\" and \"filter_valid.json\", then you can skip this code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original total: 588108\n",
      "Final total: 101220\n",
      "Data shuffle complete\n",
      "filter train data finish writing\n",
      "filter test data finish writing\n",
      "filter valid data finish writing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80976, 10121, 10121)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_dataset(['./data/train.json', './data/test.json', './data/valid.json'], './filter_dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading training data (it costs about 30~40 mins)\n",
    "* #### If you already have 'train_data.pkl', you can skip this code cell below and directly read 'train_normal_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment tokenizing...\n",
      "counting tokens...\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "token processing...\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "readdata:\n",
      "\tdata amount: 80976\n",
      "\trun time: 615.7574994564056\n",
      "size of code vocabulary:  30000\n",
      "size of comment vocabulary:  29404\n"
     ]
    }
   ],
   "source": [
    "code_voc = ['<PAD>','<START>','<END>','<UNK>']\n",
    "comment_voc = ['<PAD>','<START>','<END>','<UNK>']\n",
    "code_train, comment_train, code_voc, comment_voc = readdata('./filter_dataset/filter_train.json', code_voc, comment_voc, MODE)\n",
    "\n",
    "code_train = token2index(code_train, code_voc)\n",
    "comment_train = token2index(comment_train, comment_voc)\n",
    "code_train = pad_sequences(code_train, code_voc.index('<PAD>'))\n",
    "comment_train = pad_sequences(comment_train, comment_voc.index('<PAD>'))\n",
    "print('size of code vocabulary: ', len(code_voc))\n",
    "print('size of comment vocabulary: ', len(comment_voc))\n",
    "\n",
    "# Saving the training data:\n",
    "if MODE==\"normal\":\n",
    "    pkl_filename = \"train_normal_data.pkl\"\n",
    "elif MODE==\"simple\":\n",
    "    pkl_filename = \"train_simple_data.pkl\"\n",
    "elif MODE==\"SBT\":\n",
    "    pkl_filename = \"train_SBT_data.pkl\"\n",
    "    \n",
    "with open(pkl_filename, 'wb') as f:\n",
    "    pickle.dump([code_train, comment_train, code_voc, comment_voc], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the pickle file of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of code vocabulary:  30000\n",
      "size of comment vocabulary:  29404\n"
     ]
    }
   ],
   "source": [
    "# Getting back the training data:\n",
    "if MODE==\"normal\":\n",
    "    with open('train_normal_data.pkl', 'rb') as f:\n",
    "        code_train, comment_train, code_voc, comment_voc = pickle.load(f)\n",
    "elif MODE==\"simple\":\n",
    "    with open('train_simple_data.pkl', 'rb') as f:\n",
    "        code_train, comment_train, code_voc, comment_voc = pickle.load(f)\n",
    "elif MODE==\"SBT\":\n",
    "    with open('train_SBT_data.pkl', 'rb') as f:\n",
    "        code_train, comment_train, code_voc, comment_voc = pickle.load(f)\n",
    "    \n",
    "print('size of code vocabulary: ', len(code_voc))\n",
    "print('size of comment vocabulary: ', len(comment_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just test the functionality of transforming source code to SBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open('./data/test.json')\n",
    "inputs = input_file.readlines()\n",
    "pair = json.loads(inputs[0])\n",
    "tree = javalang.parse.parse('class aa {'+pair['code']+'}')\n",
    "print(pair['code'], end='\\n')\n",
    "\n",
    "_, node = list(tree)[2]    # 前兩個用來篩掉class aa{ }的部分\n",
    "seq = parse_tree(node, 0)\n",
    "for i in seq:\n",
    "    print(i,end='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep learing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(units):\n",
    "    return tf.keras.layers.LSTM(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = lstm(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = hidden)        \n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = lstm(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden[1], 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state_h, state_c, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units)), tf.zeros((self.batch_sz, self.dec_units))\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters and build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(code_train)\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "vocab_inp_size = len(code_voc)\n",
    "vocab_tar_size = len(comment_voc)\n",
    "\n",
    "max_length_inp = max(len(t) for t in code_train)\n",
    "max_length_targ = max(len(t) for t in comment_train)\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3)  #tensorflow 2.0\n",
    "\n",
    "\n",
    "# ==== set the checkpoint =======\n",
    "checkpoint_dir = './training_checkpoints/adam-SBT-256-60epochs'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "lossArray = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch 1637.289258480072 sec\n",
      "\n",
      "Epoch 1 Loss 0.1422\n",
      "Epoch 2 Loss 0.1383\n",
      "Epoch 3 Loss 0.1357\n",
      "Epoch 4 Loss 0.1346\n",
      "Epoch 5 Loss 0.1319\n",
      "Epoch 6 Loss 0.1285\n",
      "Epoch 7 Loss 0.1297\n",
      "Epoch 8 Loss 0.1262\n",
      "Epoch 9 Loss 0.1237\n",
      "Epoch 10 Loss 0.1238\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    hidden_h, hidden_c = encoder.initialize_hidden_state()\n",
    "\n",
    "    hidden = [hidden_h, hidden_c]\n",
    "\n",
    "    total_loss = 0 \n",
    "\n",
    "    code_train_batch = getBatch(code_train, BATCH_SIZE)\n",
    "\n",
    "    comment_train_batch = getBatch(comment_train, BATCH_SIZE)\n",
    "\n",
    "    dataset = [(code_train_batch[i], comment_train_batch[i]) for i in range(0, len(code_train_batch))]\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, hidden)\n",
    "\n",
    "            dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "\n",
    "            dec_input = tf.expand_dims([comment_voc.index('<START>')] * BATCH_SIZE, 1)       \n",
    "\n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(0, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "                dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
    "\n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        variables = encoder.variables + decoder.variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    lossArray = np.append(lossArray, (total_loss / N_BATCH) )\n",
    "\n",
    "    if epoch == 0:\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
    "\n",
    "    \n",
    "# ======= recording the hyper-parameters of the models ===========\n",
    "f_parameter = open(checkpoint_dir+\"/parameters\", \"a\")\n",
    "f_parameter.write(\"EPOCHS=\"+str(EPOCHS)+\"\\n\")\n",
    "f_parameter.write(\"BATCH_SIZE=\"+str(BATCH_SIZE)+\"\\n\")\n",
    "f_parameter.write(\"DATA=\"+MODE+\"\\n\")\n",
    "f_parameter.write(\"OPTIMIZER=\"+\"ADAM\"+\"\\n\")\n",
    "f_parameter.write(\"EMBEDDING=\"+str(EMBEDDING_DIM)+\"\\n\")\n",
    "f_parameter.write(\"UNITS=\"+str(UNITS)+\"\\n\")\n",
    "f_parameter.write(\"LOSS=\"+str(list(lossArray))+\"\\n\")\n",
    "f_parameter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learning curve of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, EPOCHS+1), lossArray, \"b\", label=\"train\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Adam Optimizer model loss\")\n",
    "plt.legend(loc = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f64c367e630>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint_dir = './training_checkpoints/adam-SBT-256-60epochs'\n",
    "#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the testing set to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./filter_dataset/filter_test.json')\n",
    "inputs = f.readlines()\n",
    "f.close()\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "for pair in inputs:\n",
    "    pair = json.loads(pair)\n",
    "    test_inputs.append(pair['code'])\n",
    "    test_outputs.append(pair['nl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model via BLEU4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "bleu4: 0.3474\n"
     ]
    }
   ],
   "source": [
    "total_bleu = 0\n",
    "for index, test in enumerate(test_inputs):\n",
    "    predict = translate(test_inputs[index], encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ, MODE)\n",
    "    bleu4_score = bleu(test_outputs[index], predict, 4)\n",
    "    total_bleu += bleu4_score\n",
    "    if (index%1000) == 0:\n",
    "        print(index)\n",
    "        \n",
    "total_bleu = total_bleu / len(test_inputs)\n",
    "print(\"bleu4:\",round(total_bleu, 4))\n",
    "\n",
    "f_parameter = open(checkpoint_dir+\"/parameters\", \"a\")\n",
    "f_parameter.write(\"BLEU4=\"+str(round(total_bleu, 4))+\"\\n\")\n",
    "f_parameter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the comment of one snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protected static String extractAccessKey(String s3uri){\n",
      "  return s3uri.substring(s3uri.indexOf(\"://\") + 3,s3uri.indexOf(':',s3uri.indexOf(\"://\") + 3));\n",
      "}\n",
      "\n",
      "Original comment:  Extracts the accessKey from the given uri\n",
      "Predictedd translation:  Parses the given file to the given label string \n",
      "bleu4: 0.0000\n"
     ]
    }
   ],
   "source": [
    "index = 8\n",
    "print(test_inputs[index], end=\"\\n\")\n",
    "print(\"Original comment: \",test_outputs[index], end=\"\\n\")\n",
    "predict = translate(test_inputs[index], encoder, decoder, code_voc, comment_voc, max_length_inp, max_length_targ, MODE)\n",
    "print(\"Predictedd translation: \", predict, end=\"\\n\")\n",
    "bleu4_score = bleu(test_outputs[index], predict, 4)\n",
    "print(\"bleu4: {:.4f}\".format(bleu4_score), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
